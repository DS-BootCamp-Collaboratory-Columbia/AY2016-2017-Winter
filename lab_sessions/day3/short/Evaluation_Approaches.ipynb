{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Approaches #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we learned about various concepts in probability theory and machine learning. We also implemented several ML models. In this lab session we'll learn about different evaluation approaches for analyzing the performance of various ML models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most commonly used measures of effectiveness in information retrieval (IR) and machine learning (ML) are precision and recall. They were originally introduced to summarize and compare search results. They are often reffered to as relevance measures. The difference between the two is in finding how well does the model perform in terms of having returned results being relevant (precision) versus returning all relevant results (recall). In IR they are defined in the following way:  \n",
    "\n",
    "Given a search query and its set of relevant documents (known a priori) the retrieval model returns a ranked list of documents, i.e. the **retrieved** documents. **Precision** is defined as the ratio between the number of retrieved and relevant documents and the total number of **retrieved** documents. It gives us the proportion of retrieved documents that are relevant. **Recall** is defined as ratio between the number of retrieved and relevant documents and the total number of **relevant** documents for that query. Recall gives us the proportion of relevant documents that are retrieved:\n",
    "$$\\Large Precision= \\frac{|Relevant\\ \\&\\ Retrieved|}{|Retrieved|}$$  \n",
    "$$\\Large Recall = \\frac{|Relevant\\ \\&\\ Retrieved|}{|Relevant|}$$  \n",
    "\n",
    "In IR precision and recall are often computed for a given ranked position. It is often the case that systems are evaluated in terms of precision or recall at a certain rank (e.g. P@10/P10, R@10/R10). Sometimes system’s performance is reported in terms of an average of the precisions (AP) at the ranks where relevant documents occur. When AP is computed across multiple test queries a mean of the AP values is reported. This measure is called mean average precision (MAP).  \n",
    "\n",
    "In ML for tasks such as classification, where the model output is not a ranked list, precision and recall are defined in terms of the true positives (tp), false positives (fp), and false negatives (fn):  \n",
    "\n",
    "$$\\Large Precision= \\frac{|tp|}{|tp+fp|}$$  \n",
    "$$\\Large Recall = \\frac{|tp|}{|tp+fn|}$$  \n",
    "\n",
    "In this case we assume that the output of the classifier is making a prediction that all retrieved documents are relevant. fp and fn are also known as Type I and Type II errors.\n",
    "Below is an illustration on how to compute Precision and Recall when evaluating a classifier:\n",
    "\n",
    "![Two Bins](precision_recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example we are going to compute precision and recall on two ranked lists of documents that were generated by two different retrieval models (model a and model b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Example ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this example we are going to evaluate the performace of two retrieval models using precision and recall. We have a test query and it's relevance set which consists of a set of documents stored in a file (3238.rel). Given the test query each model returns a ranked list of retrieved documents which are stored in two separate files (model_a.out and model_b.out). With the code below we are going to implement both measures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by reading both ranked lists (model outputs) and the relevance set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# read the output of the two models:\n",
    "model_a = np.loadtxt('model_a.out')\n",
    "model_b = np.loadtxt('model_b.out')\n",
    "\n",
    "# read the set of relevant documents\n",
    "rel_set = np.loadtxt('3238.rel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute both measures we would need to iterate over the two ranked lists and for each rank we'll check whether the ranked document is in the relevance set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rank=10\n",
    "#create lists for both models where we'll store the relevant documents \n",
    "# found in the ranked list up until the given rank:\n",
    "rank_rel_a =[]\n",
    "rank_rel_b=[]\n",
    "#iterate over the ranked list:\n",
    "for i in range(0,rank):\n",
    "    #if the ranked document is in the relevance set,\n",
    "    #add it to the list of relevant documents found for the given model:\n",
    "    if model_a[i][2] in rel_set:\n",
    "        rank_rel_a.append(i)\n",
    "    if model_b[i][2] in rel_set:\n",
    "        rank_rel_b.append (i)\n",
    "\n",
    "#compute the number of relevant and retrieved documents:\n",
    "rank_rel_a = len(np.asarray(rank_rel_a))\n",
    "rank_rel_b = len(np.asarray(rank_rel_b))\n",
    "\n",
    "#compute the number of relevant document:\n",
    "num_rel = len(rel_set)\n",
    "print(\"Number of relevant documents=\"+str(num_rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 1]**\n",
    "Given the above code your task is to compute Precision and Recall at rank 10 (rank=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 1]** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Enter your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Question 1]**  \n",
    "Change the value of the rank at which you are computing precision and recall. Start with rank=5 and continue with rank=10, 20, 30 and 40. See if you can notice a trend in how the values of Precision and Recall change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Answer 1]** Type your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihood and Perplexity ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to categorize evaluation measures is based on whether they evaluate the model on a particular task (i.e. extrinsically), which requires ground truth information of the model performance as we noticed in the previous example when computing precision and recall. Another way to evaluate the performance of the model is based on how well the model represents the collection (i.e. intrinsically). In many instances due to the nature of the model and/or due to lack of well defined extrinsic task, models are evaluated based on how well they could predict a held-out set. One commonly used intrinsic evaluation approach is to measure the log-likelihood of the held out set (i.e. the test set).  \n",
    "\n",
    "Given a held out set of $n$ data points $x_i$ and the trained model parameters  $\\theta$, log-likelihood is computed as the sum of the log-likelihood of the data points given the model parameters $p(x_i|\\theta)$:  \n",
    "\t\t$$\\mathcal{L}(x)\\ =\\ \\sum_{i=1}^{n} \\log p(x_i|\\theta)$$\n",
    "\n",
    "For many ML and natural language processing (NLP) models Perplexity is used rather than log-likelihood. Perplexity is defined as:\n",
    "\n",
    "$$ perplexity(x)\\ =\\ \\exp \\Bigg\\{-\\frac{\\mathcal{L}(x)}{|n|} \\Bigg\\} $$\n",
    "\n",
    "In the following example we are going to use perplexity to evaluate topic models. Earlier today we learned about topic models and we used a topic model configuration with 100 topics (T=100) to infer topics on a NYT article. In the following example we are going to evaluate that model using perplexity. In order to do that we are going to first derive the  perplexity expression for topic models.  \n",
    "In topic models we are dealing with documents $d$ and words $w$. The model gives us the per document-topic distributions and per topic-word distributions. For a test set of  $M$ documents each  of which containing $N_d$ words perplexity is defined as:\n",
    "\n",
    "$$ perplexity(D)\\ =\\ \\exp \\Bigg\\{-\\frac{\\sum_{d=1}^M{\\log p(w_d)}}{\\sum_{d=1}^M{N_d}} \\Bigg\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Computation Example ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kriste/anaconda/envs/p35/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "nytimes = []\n",
    "with open('nytimes_100docs.txt') as inputfile:\n",
    "    for line in inputfile:\n",
    "        nytimes.append(line.lower().split())\n",
    "\n",
    "# obtain the frequency of each word\n",
    "frequency = collections.defaultdict(int)\n",
    "for doc in nytimes:\n",
    "    for token in doc:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# obtain the frequency of the words as a numpy array\n",
    "n_most_common = 25\n",
    "np_freq = np.zeros(len(frequency))\n",
    "count = 0\n",
    "for token in frequency:\n",
    "    np_freq[count] = frequency[token]\n",
    "    count += 1\n",
    "# sort the frequencies\n",
    "np_freq_sorted = np.sort(np_freq)\n",
    "# obtain the maximum allowed frequency\n",
    "max_freq = np_freq_sorted[-n_most_common]\n",
    "\n",
    "# remove words that appear only once or more than M times\n",
    "doc_noLowFreq = [[token for token in text if frequency[token] > 1 and frequency[token]<max_freq]\n",
    "                  for text in nytimes]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(doc_noLowFreq)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_noLowFreq]\n",
    "#Split the collection into 80% for training and 20% for test (held out set)\n",
    "train_corpus = corpus[:80]\n",
    "test_corpus = corpus[80:]\n",
    "T=100\n",
    "model = gensim.models.LdaModel(train_corpus, id2word=dictionary, num_topics=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 2]** Given that we've trained a LDA model using 80% of the corpus as a training set compute the log-likelihood of the test set using the gensim method \"model.log_perplexity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 2]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Enter your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance Testing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll implement and explore the randomization test which is also known as the permutation test. The randomization test helps us determine whether the difference in the test statistic used to judge the two models/systems is statistically significant or not, i.e. whether the difference happened by chance (e.g. a noise introduced in the evaluation process). We have two evaluation results obtained from averaging a certain evaluation measure across the points in our test set using two models. When we compare these two models what we typically do in order to determine which model performs better is take the difference between the evaluation results (i.e. test statistics).  \n",
    "\n",
    "In the randomization test the null hypothesis states that the results of the two models across the data points in the test set were generated by the same underlying process and therefore there is no difference in the performance of the two models. When computing the randomization test, the first step is to generate the distribution of the test statistics under the null-hypothesis.We go about doing that by generating two evaluation results for the two models using the following procedure:  \n",
    "(1) Go over each data point in the evaluation set  \n",
    "(2) Randomly choose an evaluation result from the two model results for that point  \n",
    "(3) Repeat the process for the two models and compute the mean for each of the newly generated sets of evaluation results  \n",
    "(4) Compute the difference between the two means (i.e. the test statistic)  \n",
    "(5) Repeat steps 1 through 4 n times and store each test statistic. These steps are often referred to as creating the distribution of the test statistic under the null hypothesis.   \n",
    "\n",
    "At the end go over the n generated test statistics and count the number of times their values were larger than our original test statistic. The p-value is defined as the probability of having the test statistics generated under the null hypothesis be higher than our original test statistic. It is computed by dividing the number of times the test statistic under the null hypothesis was higher than our original test statistic with the total number of times we computed the test statistics under the null hypothesis n. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to implement the randomization test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# read the output of the two models:\n",
    "model_1 = np.loadtxt('model_f.res')\n",
    "model_2 = np.loadtxt('model_e.res')\n",
    "\n",
    "perf_model_1 = model_1[:,1].mean()\n",
    "perf_model_2 = model_2[:,1].mean()\n",
    "diff = perf_model_a - perf_model_b\n",
    "print (\"Model 1 MAP is=\"+str(round(perf_model_1,4)))\n",
    "print (\"Model 2 MAP is=\"+str(round(perf_model_2,4)))\n",
    "print (\"Difference in performance is:\")\n",
    "print (\"MAP(1)-MAP(2)=\"+str(round(diff,4)))\n",
    "\n",
    "#first we are going to concatanate the two results\n",
    "all_res = np.append(model_1[:,1], model_2[:,1])\n",
    "#next we set the number of samples\n",
    "iter=10000\n",
    "iter_val=np.zeros(iter)\n",
    "for j in range(0,iter):\n",
    "    perm = np.random.permutation(np.arange(len(all_res)))\n",
    "    tr1 =all_res[perm[:len(model_1)]]\n",
    "    tr2 =all_res[perm[len(model_2):]]\n",
    "    iter_val[j]=tr1.mean()-tr2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assingment 3]** Use the above outlined steps and compute the p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 3]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#type your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 4]** We just used the randomization test to determine how significant the performance difference is between models \"e\" and \"f\". Aside from the results of these two models we also provided you with results from two models model \"c\" and model \"d\". Use the randomization test to perform the same significance testing over the other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Coefficients ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many data science problems we are often tasked with finding the relationship between ranked lists. For example, these ranked lists could contain the performance of a particular model across different configurations of the same parameter. To give a concrete example, let's say we have 4 different types of topic models and each model is evaluated using 7 different topic configurations (T=20,30,40,50,60,70 and 80). Given two such ranked lists we need to come up with a decision whether model A ranks the performance of the different settings similarly to model B. In other words we need to quantify the correlation between the two ranked lists. We do that using correlation coefficients. In order to measure the strength of the linear relationship we use linear correlation coefficients such as Pearson coefficient (often referred to as Pearson's $R$). When the goal of our correlation analysis is the relationship between the rankings of different models we turn to rank correlation coefficients such as Spearman’s rank correlation coefficient ($\\rho$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example ###\n",
    "In this example we are going to compute the Pearson's ($R$) and the Spearman's ($\\rho$) coefficients across 4 ranked lists. Each rank list contains results of the performance of one of the 4 different ML models across 7 different parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  3.  4.  5.  6.  7.]\n",
      "[ 1.  6.  7.  2.  3.  4.  5.]\n",
      "[ 1.  2.  3.  4.  5.  6.  7.]\n",
      "[ 2.  3.  5.  6.  1.  7.  4.]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "m1 = [0.0583 , 0.1521 , 0.2187 , 0.2562 , 0.3042 , 0.3542 , 0.3564]\n",
    "m2 = [0.0274 , 0.1428 , 0.2010 , 0.0807 , 0.0854 , 0.1102 , 0.1155]\n",
    "m3 = [0.1669 , 0.1715 , 0.2551 , 0.2729 , 0.2750 , 0.3170 , 0.3182]\n",
    "m4 = [0.1154 , 0.2099 , 0.2878 , 0.3412 , 0.1124 , 0.4671 , 0.2723]\n",
    "\n",
    "#Convert the above values into ranks:\n",
    "rm1 = scipy.stats.rankdata(m1)\n",
    "rm2 = scipy.stats.rankdata(m2)\n",
    "rm3 = scipy.stats.rankdata(m3)\n",
    "rm4 = scipy.stats.rankdata(m4)\n",
    "\n",
    "print (rm1)\n",
    "print (rm2)\n",
    "print (rm3)\n",
    "print (rm4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 5]**  \n",
    "Use the first rank list (rm1) as a reference and compute Pearson's and Spearman's correlation coefficients with the other 3 ranked lists. Use the **scipy.stats** implementation of these two correlation coefficients: **scipy.stats.pearsonr** and **scipy.stats.spearmanr**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 5]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute the Pearson's correlation coefficient here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute the Spearman's correlation coefficient here."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:p35]",
   "language": "python",
   "name": "conda-env-p35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
