{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Topic Models\n",
    "\n",
    "In this module, we will learn how to apply and visualize topic models in Python. We will use the package `gensim` (see [here](https://radimrehurek.com/gensim) for further information and tutorials.)\n",
    "\n",
    "First of all, we import the package, along with `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "We start with a toy example to illustrate how to preprocess and visualize data. Consider a set of four documents, each consisting of one single sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc1 = \"I like to eat broccoli and bananas. Broccoli and bananas are healthy.\"\n",
    "doc2 = \"I eat broccoli smoothie and bananas for breakfast.\"\n",
    "doc3 = \"Hamsters and kittens are cute.\"\n",
    "doc4 = \"My sister says she wants to adopt two cute kittens, but we already have three hamsters at home.\"\n",
    "\n",
    "# complete list of documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps that we will go through:\n",
    "1. Remove punctuation.\n",
    "2. Remove \"stop words\".\n",
    "3. Remove low-frequency words.\n",
    "4. Create the dictionary.\n",
    "5. Create the bag-of-words representation.\n",
    "\n",
    "### 1. Remove punctuation\n",
    "\n",
    "First, we remove the punctuation signs (commas, periods, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "print(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_noPunc = [''.join(ch for ch in doc if ch not in exclude) for doc in doc_complete]\n",
    "print(doc_noPunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove \"stop words\"\n",
    "\n",
    "In computing and natural language processing, stop words are words which are filtered out as a pre-processing step. Stop words are usually common words that are *semantically insignificant* (e.g., articles like \"the\" or \"a\"). The specific list of stop words will vary from application to application; indeed, some applications do not remove any stop word.\n",
    "\n",
    "In our toy example, we will use the following list of stop words:\n",
    "```\n",
    "stoplist = set('i my to and a for are at this on of she or but we'.split())\n",
    "```\n",
    "Of course, in more realistic applications, we do not specify the stop words by hand. Rather, we use existing lists for that purpose. Two examples of stop word lists in English are:\n",
    "- http://xpo6.com/list-of-english-stop-words\n",
    "- http://www.textfixer.com/resources/common-english-words.txt\n",
    "\n",
    "Additionally, the package [`nltk`](http://www.nltk.org/book/ch02.html) contains stopword lists in several languages.\n",
    "\n",
    "In the cell below, remove the stop words from the document list `doc_complete` using the `stoplist` defined above. Do not forget to *lowercase* all words before comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('i my to and a for are at this on of she or but we'.split())\n",
    "doc_noStop = [[word for word in document.lower().split() if word not in stoplist]\n",
    "               for document in doc_noPunc]\n",
    "print(doc_noStop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove high and low-frequency words\n",
    "\n",
    "High and low-frequency words are typically removed or simply ignored, because they will not provide any useful information (none of them are discriminative). In the cell below, we first count how many times each term appears. We also remove all words that appear only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "# obtain the frequency of each word\n",
    "frequency = collections.defaultdict(int)\n",
    "for doc in doc_noStop:\n",
    "    for token in doc:\n",
    "        frequency[token] += 1\n",
    "# remove words that appear only once\n",
    "doc_noLowFreq = [[token for token in text if frequency[token] > 1]\n",
    "                  for text in doc_noStop]\n",
    "print(doc_noLowFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove high-frequency words, we could remove the top (say) 25 most common words by replacing\n",
    "```python\n",
    "# remove words that appear only once\n",
    "doc_noLowFreq = [[token for token in text if frequency[token] > 1]\n",
    "                  for text in doc_noStop]\n",
    "```\n",
    "with\n",
    "```python\n",
    "# obtain the frequency of the words as a numpy array\n",
    "n_most_common = 25\n",
    "np_freq = np.zeros(len(frequency))\n",
    "count = 0\n",
    "for token in frequency:\n",
    "    np_freq[count] = frequency[token]\n",
    "    count += 1\n",
    "# sort the frequencies\n",
    "np_freq_sorted = np.sort(np_freq)\n",
    "# obtain the maximum allowed frequency\n",
    "max_freq = np_freq_sorted[-n_most_common]\n",
    "\n",
    "# remove words that appear only once or more than max_freq times\n",
    "doc_noLowFreq = [[token for token in text if frequency[token] > 1 and frequency[token]<max_freq]\n",
    "                  for text in doc_noStop]\n",
    "```\n",
    "\n",
    "We will not do that in this tutorial because this is a toy example with a few vocabulary words.\n",
    "\n",
    "\n",
    "### 4. Create the dictionary\n",
    "\n",
    "In topic modeling, we represent documents using the approach called \"bag-of-words\". This consists in two steps. First, each vocabulary word is assigned a unique integer id. Second, each document is represented by a vector, where the $n$-th element in that vector contains the number of times that the word with id $n$ appears in the document.\n",
    "\n",
    "The mapping between the vocabulary words and the ids is called a *dictionary*. Below, we create a dictionary using the `gensim` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(doc_noLowFreq)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `token2id()` allows us to recover the id that was assigned to each vocabulary word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create the bag-of-words representation\n",
    "\n",
    "The function `doc2bow()` counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. For instance,\n",
    "\n",
    "```\n",
    "   dictionary.doc2bow(\"kittens are cute and hamsters are also cute\".lower().split())\n",
    "```\n",
    "\n",
    "returns `[(3, 2), (4, 1), (5, 1)]` to indicate that word with id 3 (\"cute\") appears twice, word with id 4 (\"hamsters\") appears once, and word with id 5 (\"kittens\") appears also once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dictionary.doc2bow(\"kittens are cute and hamsters are also cute\".lower().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the `doc2bow()` function to our collection of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in doc_noLowFreq]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appying LDA\n",
    "\n",
    "We now apply Latent Dirichlet Allocation (LDA) to our preprocessed corpus. The idea behind LDA is that each document can be understood as a mixture of \"topics\". For instance, documents 1 and 2 are about food because they contain the words \"broccoli\", \"bananas\", and \"eat\"; documents 3 and 4 are about animals (\"kittens\", \"hamsters\", \"cute\"); and document 5 is about both animals (\"hamsters\") and food (\"broccoli\"). LDA unveils these topics automatically from the data.\n",
    "\n",
    "We use the package `gensim` to fit LDA. We specify the dictionary and the number of topics that we want to find. In this case, we use only 2 topics (typical values in a realistic scenario will be 50-200 topics). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the topics\n",
    "\n",
    "We can print the topics found by the LDA model using the function `print_topics`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, one of the topics (e.g., topic 0) will mainly express the words \"broccoli\", \"eat\", and \"bananas\" with higher percentage, whereas the other topic (e.g., topic 1) will be mostly about \"cute\", \"hamsters\", and \"kittens\". This is consistent with our earlier intuitions of having a topic about animals and another topic about food.\n",
    "\n",
    "Recall that a topic is formally defined as a distribution over the entire vocabulary.\n",
    "\n",
    "### Obtain the topic proportions\n",
    "\n",
    "We now want to find the topic proportions of each individual document. For instance, we know that document 1 is mostly about food, while document 4 is mostly about animals. The following commands allow us to obtain the topic distribution of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model[dictionary.doc2bow(doc1.split())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model[dictionary.doc2bow(doc4.split())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this can be applied to unseen documents too. For instance, consider the following new document, which is about both animals and food:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc5 = \"Look at these hamsters munching on a piece of broccoli\".lower()\n",
    "print(model[dictionary.doc2bow(doc5.split())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting topic proportions should be around $0.5$ (at least moderately close; keep in mind that these are all very short documents), indicating that this document expresses both topics."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
