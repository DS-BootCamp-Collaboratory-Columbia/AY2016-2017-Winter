{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling of the New York Times\n",
    "\n",
    "We will use a subset of articles from the New York Times dataset (downloaded from https://archive.ics.uci.edu/ml/datasets/Bag+of+Words).\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "We will load the data into the variable `nytimes` using the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nytimes_30000docs.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-32201786647e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnytimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nytimes_30000docs.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minputfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mnytimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nytimes_30000docs.txt'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "nytimes = []\n",
    "with open('nytimes_30000docs.txt') as inputfile:\n",
    "    for line in inputfile:\n",
    "        nytimes.append(line.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "\n",
    "The dataset has already been preprocessed to remove punctuation and stop words. It contains one document per line, and words within documents have been ordered alphabetically (this doesn't affect the bag-of-words representation).\n",
    "\n",
    "### Remove high and low-frequency words\n",
    "\n",
    "**[Task]** Use the cell below to remove the words that appear less than 5 times in the corpus and also remove the 25 most common words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dictionary\n",
    "\n",
    "**[Task]** Create the dictionary in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the bag-of-words representation\n",
    "\n",
    "**[Task]** Use the cell below to obtain the bag-of-words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA\n",
    "\n",
    "**[Tasks]**\n",
    "\n",
    "1. Fit an LDA model with `num_topics=100` to the NYT dataset.\n",
    "\n",
    "2. Plot the dominant topics using the function `print_topics(num_topics=20, num_words=10)`. Read the words in some of the topics. Does the result make sense? Can you think of a \"title\" that summarizes each topic?\n",
    "\n",
    "3. Go to the New York Times website. Choose a section and find an article in that section. Copy the article (document) into a single line and remove punctuation marks. In the ipython notebook, obtain the topic proportions that this article (document) exhibits, and plot a bar chart with these proportions. Show the top 10 words of the top 2 topics of that article using the function `print_topic(topicno)`.\n",
    "\n",
    "**[Warning]** Fitting LDA to this dataset on a laptop may take 2-3 minutes, so be patient. However, if it takes longer than 5 minutes, you may use the dataset with 5000 documents instead of the dataset with 30000 documents. Just replace the corresponding line on the initial cell of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
