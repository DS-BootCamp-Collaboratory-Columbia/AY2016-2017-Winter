{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import array as arr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In this lab we explore fitting a linear regression to data using *ordinary least squares* (OLS).\n",
    "\n",
    "We use simulated data for practice. The data $X$ are simulated as follows:\n",
    "\n",
    "$Y = 0.1 X^3 + 0.5 X^2 + 10.2 X - 21 + \\epsilon \\;\\;$ where $\\epsilon \\sim \\mathcal{N}(0,5)$\n",
    "\n",
    "In words, the dependent variable equals a third order polynomial (cubic) of the one-dimensional independent variable plus Gaussian noise. Below is the cubic $f$ and the function  *sim_data* that simulates the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x): return 0.1*x**3 + 0.5*x**2 + 10.2*x - 21.\n",
    "\n",
    "def sim_data(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    min_x, max_x, stepsize = -10, 10, 0.1\n",
    "    #independent var (does not have to be uniformly space in general):\n",
    "    X = arr([np.arange(min_x, max_x, stepsize)]).T\n",
    "    (N,_) = X.shape\n",
    "    Y = f(X) + np.random.normal(0,5,size=np.shape(X)) #dependent var\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "Let's simulate some data, $X,Y$ and plot it as a scatter. By convention, the response/dependent variable goes on the y-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,Y = sim_data()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.scatter(X,Y,marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an intercept column to the independent variables\n",
    "We add a new column of ones to the independent array X that allows OLS to use an intercept/systematic bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(N,D) = X.shape\n",
    "X1 = np.hstack((np.ones((N,1)), X))\n",
    "print('X1 (first 10 rows):\\n',X1[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares\n",
    "\n",
    "Recall the basic assumption of linear regression:\n",
    "${\\bf Y} = {\\bf X}^\\top \\beta + \\epsilon$ where $\\epsilon$ is drawn i.i.d. from a normal distribution. \n",
    "\n",
    "OLS fits this model by minimizing the sum of squared errors in linear regression. Unlike many inferentialalgorithms that you will encounter, its solution is closed-form (has a single step that can be written down explicitly in maths).\n",
    "\n",
    "OLS calculates the unknown coefficients $\\beta$ from data as follows:\n",
    "\n",
    "$\\beta = ({\\bf X}^\\top {\\bf X})^{-1} {\\bf X}^\\top {\\bf Y}$\n",
    "\n",
    "We next calculate $\\beta$ using this equation. \n",
    "However, be aware that it is better to avoid matrix inversions in practice, as it can be a numerically unstable operation. In this case, the premultiplier to ${\\bf Y}$ is known as the Moore-Penrose *psuedoinverse* of ${\\bf X}$:\n",
    "\n",
    "${\\bf M} = ({\\bf X}^\\top {\\bf X})^{-1} {\\bf X}^\\top$\n",
    "\n",
    "and there is a function in the ```numpy``` package that can calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = np.linalg.pinv(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: calculating the coefficients\n",
    "\n",
    "Use the pseudoinverse ${\\bf M}$ to calculate the coefficients $\\beta$ in the code cell below. Then use $\\beta$ to calculate the predictions of linear regression given the matrix ```X1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now plot the original data and the linear regression fit:\n",
    "print('coefficients:\\n',beta)\n",
    "plt.scatter(X1[:,1],Y,marker='x')\n",
    "plt.plot(X1[:,1],Y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_lst, sum_residuals, rank, s = np.linalg.lstsq(X1, Y)\n",
    "print('coefficients using np.linalg.lstsq:\\n',beta_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals\n",
    "The *residuals* of a regression fit are the actual Y minus the predicted Y. This is the number being minimized by OLS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "residuals = (Y - Y_prediction)\n",
    "SR = residuals**2\n",
    "print('sum of squared residuals', SR.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to plot the residuals as a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(residuals)\n",
    "plt.xlabel('error')\n",
    "plt.ylabel('frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the shape of this histogram: there are many more residuals less than zero than there are residuals greater than zero. This is interesting because the linear regression model assumes that the error is drawn from a Gaussian with zero mean. Something is going here (discussed in the next section of this lab).\n",
    "\n",
    "### Explained variance\n",
    "As covered in the videos for this day, we can also calculate the variance explained by the model as proportion of the total variance of the dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_var = Y.var()\n",
    "explained_var = total_var - residuals.sum()/float(N)\n",
    "r2 = explained_var / total_var\n",
    "print('total_var, explained_var, r-squared:\\n', total_var, explained_var, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting error independence\n",
    "Finally, in this section we look at the residual error as a function of the independent variable. In the model specification, the errors should be *independently* distributed, but this cart below shows that this is not the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X,residuals)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('residual error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Mismatch and Higher Order Basis Functions\n",
    "\n",
    "The above linear fit to obviously non-linear data is an example of model mismatch. It leads to the data violating some of the assumptions of linear regression, namely that the errors are independently and identically distributed to a Gaussian. \n",
    "\n",
    "Sometimes we know the source of model mismatch and are able to extend the model to better fit the data. In general, this is often not possible. However, in this case, we know exactly the source of model mismatch and can get a better fit with linear regression if we add quadratic ($x^2$) and cubic ($x^3$) basis functions to the independent variable. \n",
    "\n",
    "### Task:\n",
    "Add quadratic and cubic functions of the data X below, following the same steps of analysis as above and see if you can obtain a better fit. \n",
    "\n",
    "Do you notice anything about the shape of the error histogram this time? Why does it differ to the shape of the error histogram for linear regression using only the zero-th and first order powers of X?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X,Y,marker='x')\n",
    "plt.plot(X,Y_prediction_cubic)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(err_cubic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X,err_cubic)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('error')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
