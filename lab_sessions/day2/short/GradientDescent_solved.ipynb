{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Optimization algorithms are the cornerstone of machine learning, because many learning tasks can be recast as an optimization problem.\n",
    "\n",
    "One of the most widely spread optimization routines is gradient descent (also known as steepest descent), which is an iterative optimization method. Gradient descent seeks to *find a local minimum of a function* using only the information from the gradient (i.e., it performs first-order optimization). If we are *maximizing* a function instead of minimizing, we use gradient ascent.\n",
    "\n",
    "The idea behing gradient descent is simple. It relies on the fact that *the gradient of a function points in the direction of the greatest rate of increase of that function*. Thus, if we follow the negative gradient, we will walk towards the direction of the greatest rate of decrease. By following the negative gradient, we will end up converging to a (local) minimum of the function. In contrast, by following the gradient, we will converge to a (local) maximum of the function.\n",
    "\n",
    "Consider that we want to minimize a function $f(x)$ with respect to the variable $x$. The variable $x$ can be a vector. The gradient of the function, $\\nabla_x f(x)$, is a vector of the same length of $x$ that points to the direction of steepest ascent. Assume that we have an initial guess $x_0$ of where the minimum might be (in practice, the initial guess $x_0$ can be chosen depending on the available information, or even randomly). We can refine $x_0$ by following the negative gradient at that point, $\\nabla_x f(x_0)$:\n",
    "$$ x_1 = x_0-\\rho_0 \\nabla_x f(x_0) $$\n",
    "Here, $\\rho_0$ is a scalar that denotes the stepsize. For now, let us simply consider that $\\rho_0$ is \"small enough.\" Then, $x_1$ can be closer to the actual local minimum than the initial guess. We can apply this process iteratively to further refine the solution as\n",
    "$$ x_{t+1} = x_t-\\rho_t \\nabla_x f(x_t) $$\n",
    "\n",
    "## Example 1\n",
    "\n",
    "As a simple initial example, consider that $x=[x_0, x_1]^\\top$ is a 2-dimensional vector, and the function $f(x)$ is given by\n",
    "$$ f(x) = \\left(x_1-\\frac{5.1}{4\\pi^2}x_0^2+\\frac{5}{\\pi}x_0-6\\right)^2+10\\left(1-\\frac{1}{8\\pi}\\right)\\cos(x_0)+10.$$\n",
    "This complicated function is called \"Branin function.\"\n",
    "\n",
    "**Visualize how this function looks like.** The code below:\n",
    "\n",
    "- Evaluates $f(x)$.\n",
    "- Defines auxiliary python functions for plotting.\n",
    "- Evaluates $f(x)$ on a grid of points and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import useful packages\n",
    "from math import pi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "\n",
    "# Define the function f(x) for 2-dimensional vector x\n",
    "def f(x):\n",
    "    t = 1.0/(8*pi)\n",
    "    s = 10.0\n",
    "    r = 6.0\n",
    "    c = 5.0/pi\n",
    "    b = 5.1/(4*pi**2)\n",
    "    a = 1.0\n",
    "    term1 = a*(x[1] - b*x[0]**2 + c*x[0] - r)**2\n",
    "    term2 = s*(1.0-t)*np.cos(x[0])\n",
    "    return(term1 + term2 + s)\n",
    "\n",
    "# Define auxiliary functions for plotting\n",
    "def plot_surf(X, Y, Z):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, \n",
    "                           cmap=plt.cm.RdBu, linewidth=0, antialiased=False)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    plt.xlabel('$x_0$')\n",
    "    plt.ylabel('$x_1$')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_contour(X, Y, Z, xpath=None, ypath=None):\n",
    "    fig = plt.figure()\n",
    "    cs = plt.contourf(X, Y, Z, 100, cmap=plt.cm.RdBu)\n",
    "    fig.colorbar(cs, shrink=0.5, aspect=5)\n",
    "    plt.xlabel('$x_0$')\n",
    "    plt.ylabel('$x_1$')\n",
    "    if xpath is not None:\n",
    "        plt.plot(xpath, ypath)\n",
    "    plt.plot([-pi,pi,9.42478],[12.275,2.275,2.475],'kx')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a grid on the region -5<x0<12, 0<x1<15\n",
    "Xlin = np.linspace(-5,12,100)\n",
    "Ylin = np.linspace(0,15,100)\n",
    "# Grid of points (x,y)\n",
    "Xgrid, Ygrid = np.meshgrid(Xlin, Ylin)\n",
    "\n",
    "# Evaluate the function f on all the point of the grid     \n",
    "Zgrid = np.array([[f((x,y)) for x in Xlin] for y in Ylin])\n",
    "\n",
    "# Surface plot\n",
    "plot_surf(Xgrid, Ygrid, Zgrid)\n",
    "\n",
    "# Contour plot\n",
    "plot_contour(Xgrid, Ygrid, Zgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local minima.** This function has three minima: $x=(-\\pi,12.275)$, $x=(\\pi,2.275)$ and $x=(9.42478,2.475)$. It turns out that the three local minima are also global minima, since the three of them reach the same function value of $f(x)=0.397887$.\n",
    "\n",
    "**Gradient (mathematical expression).** But for now, let us assume that we are not aware of this information, and we want to minimize the function using gradient ascent. Indeed, real-world examples have higher dimensionality and we will not be able to make these plots. For that, we first obtain the gradient of the function, which is given by\n",
    "$$\n",
    "\\nabla_x f(x) =\n",
    "\\left[ \n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial f(x)}{\\partial x_0} \\\\\n",
    "\\frac{\\partial f(x)}{\\partial x_1} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[ \n",
    "\\begin{array}{c}\n",
    "2\\left(x_1-\\frac{5.1}{4\\pi^2}x_0^2+\\frac{5}{\\pi}x_0-6\\right)\\left( -\\frac{5.1}{2\\pi^2}x_0+\\frac{5}{\\pi} \\right)-10\\left(1-\\frac{1}{8\\pi}\\right)\\sin(x_0) \\\\\n",
    "2\\left(x_1-\\frac{5.1}{4\\pi^2}x_0^2+\\frac{5}{\\pi}x_0-6\\right) \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "**Gradient (python).** Here is the function that evaluates the gradient of $f(x)$ at any point $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient evaluation\n",
    "def grad_f(x):\n",
    "    t = 1.0/(8*pi)\n",
    "    s = 10.0\n",
    "    r = 6.0\n",
    "    c = 5.0/pi\n",
    "    b = 5.1/(4*pi**2)\n",
    "    a = 1.0\n",
    "    term1 = 2.0*a*(x[1] - b*x[0]**2 + c*x[0] - r)\n",
    "    term2 = -s*(1.0-t)*np.sin(x[0])\n",
    "    \n",
    "    y = np.zeros((2,))\n",
    "    y[0] = term1*(-2.0*b*x[0] + c) + term2\n",
    "    y[1] = term1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial guess.** We start with an initial guess of, say, $x_0 = (0,0)$.\n",
    "\n",
    "**Algorithm.** At each iteration $t$ of the gradient descent algorithm, we set\n",
    "$$x_{t+1} = x_t-\\rho_t \\nabla_x f(x_t).$$\n",
    "For simplicity, we assume a constant step size $\\rho_t=\\rho$ for all iterations.\n",
    "\n",
    "**Convergence.** How many iterations should we run? There is not a unique answer for that. One standard approach is to stop when the gradient has norm of approximately zero (this indicates that we are near the optimum). Another approach is to stop when $x_{t+1}$ and $x_t$ are approximately equal (this indicates that we are barely moving away from that point). We will follow the former approach. It is also common to stop the algorithm after reaching a pre-specified number of iterations to prevent infinite loops.\n",
    "\n",
    "**Implementation of gradient descent in Python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the initial guess\n",
    "x = np.zeros(2)\n",
    "\n",
    "# Set the step size (constant in this case)\n",
    "rho = 0.1\n",
    "\n",
    "# Define the tolerance (to declare convergence)\n",
    "tol = 1.0e-6\n",
    "\n",
    "# Specify the maximum number of iterations\n",
    "max_iterations = 1000\n",
    "\n",
    "# Iteration counter\n",
    "it = 0\n",
    "\n",
    "# While not converged, repeat\n",
    "while(it==0 or np.sqrt(np.sum(grad**2))>tol and it<max_iterations):\n",
    "    # Evaluate the gradient\n",
    "    grad = grad_f(x)\n",
    "    # Refine the initial guess\n",
    "    x = x - rho*grad\n",
    "    # Increase the iteration number\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Run the code above. In the cell below, print the value of the optimum $x$ found, as well as the value of $f(x)$ evaluated at that point. Does this point correspond to any of the global optima described above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Value of the function after convergence: ' + str(f(x)))\n",
    "print('Value of x at the optimum: ' + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting the optimization path.** Now, keep track of *optimization path* to visualize what gradient ascent is actually doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lists to save the optimization path\n",
    "x0_path = []\n",
    "x1_path = []\n",
    "\n",
    "# Set the step size (constant in this case)\n",
    "rho = 0.1\n",
    "\n",
    "# Initial value\n",
    "x = (0.0,0.0)\n",
    "\n",
    "# Append the initial value to the path\n",
    "x0_path.append(x[0])\n",
    "x1_path.append(x[1])\n",
    "\n",
    "# Specify the maximum number of iterations\n",
    "max_iterations = 1000\n",
    "\n",
    "# Iteration counter\n",
    "it = 0\n",
    "\n",
    "# While not converged, repeat\n",
    "while(it==0 or np.sqrt(np.sum(grad**2))>tol and it<max_iterations):\n",
    "    # Evaluate the gradient\n",
    "    grad = grad_f(x)\n",
    "    # Refine the initial guess\n",
    "    x = x - rho*grad\n",
    "    # Append the value to the path\n",
    "    x0_path.append(x[0])\n",
    "    x1_path.append(x[1])\n",
    "    # Increase the iteration number\n",
    "    it += 1\n",
    "\n",
    "plot_contour(Xgrid, Ygrid, Zgrid, x0_path, x1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Tasks]** Based on the code above, perform the following tasks and answer these questions:\n",
    "\n",
    "1. Play with *smaller* step sizes and visualize the optimization path in each case. Does the path look smoother or sharper? Why?\n",
    "\n",
    "2. Play with *slightly larger* values of the step size ($\\rho=0.2$, $0.3$, and $0.4$). What happens?\n",
    "\n",
    "3. Set $\\rho=0.1$ again. Initialize the algorithm at $x=(6.0,14.0)$. Plot the optimization path.\n",
    "\n",
    "4. Think of a setting (initial guess and step size) such that the solution is $x=(-\\pi,12.275)$. How can you get this solution? Verify that by plotting the optimization path.\n",
    "\n",
    "5. Set the initial guess at $x=(-2.0,10.0)$ and the step size as $\\rho=0.089$. Run the code again. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Here, we only include the solution for question 1. The rest of the questions are similar ###\n",
    "\n",
    "# Lists to save the optimization path\n",
    "x0_path = []\n",
    "x1_path = []\n",
    "\n",
    "# Set the step size (constant in this case)\n",
    "rho = 0.01    # <--- Changed only this\n",
    "\n",
    "# Initial value\n",
    "x = (0.0,0.0)\n",
    "\n",
    "# Append the initial value to the path\n",
    "x0_path.append(x[0])\n",
    "x1_path.append(x[1])\n",
    "\n",
    "# Specify the maximum number of iterations\n",
    "max_iterations = 1000\n",
    "\n",
    "# Iteration counter\n",
    "it = 0\n",
    "\n",
    "# While not converged, repeat\n",
    "while(it==0 or np.sqrt(np.sum(grad**2))>tol and it<max_iterations):\n",
    "    # Evaluate the gradient\n",
    "    grad = grad_f(x)\n",
    "    # Refine the initial guess\n",
    "    x = x - rho*grad\n",
    "    # Append the value to the path\n",
    "    x0_path.append(x[0])\n",
    "    x1_path.append(x[1])\n",
    "    # Increase the iteration number\n",
    "    it += 1\n",
    "\n",
    "plot_contour(Xgrid, Ygrid, Zgrid, x0_path, x1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "Consider the following function of three variables,\n",
    "$$\n",
    "f(x) =  \\frac{1}{2}\\left( x_0^4+ x_1^4 + x_2^4 \\right) -8\\left( x_0^2+x_1^2+x_2^2 \\right) +\\frac{5}{2}\\left(x_0 +x_1+ x_2\\right),\n",
    "$$\n",
    "with gradient\n",
    "$$\n",
    "\\nabla_x f(x) =\n",
    "\\left[ \n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial f(x)}{\\partial x_0} \\\\\n",
    "\\frac{\\partial f(x)}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f(x)}{\\partial x_2}\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[ \n",
    "\\begin{array}{c}\n",
    "2x_0^3-16x_0+\\frac{5}{2} \\\\\n",
    "2x_1^3-16x_1+\\frac{5}{2} \\\\\n",
    "2x_2^3-16x_2+\\frac{5}{2}\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "(This is called the Styblinski-Tang function.)\n",
    "\n",
    "**[Task]** Implement a gradient descent method to find a local minimum of this function. Use a step size of $\\rho=0.01$. How many different local optima can you find?\n",
    "\n",
    "*Note: You do not need to plot the optimization path here. In general, we do not plot the optimization path when the dimensionality of $x$ is larger than 2.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Here, we only include the solution to find only one local optimum. To find others, simply change the initial guess ###\n",
    "\n",
    "# Function evaluation\n",
    "def f2(x):\n",
    "    return 0.5*np.sum(x**4)-8.0*np.sum(x**2)+2.5*np.sum(x)\n",
    "\n",
    "# Gradient evaluation\n",
    "def grad_f2(x):\n",
    "    y = 2.0*x**3-16.0*x+2.5\n",
    "    return y\n",
    "\n",
    "# Set the step size (constant in this case)\n",
    "rho = 0.01\n",
    "\n",
    "# Define the tolerance (to declare convergence)\n",
    "tol = 1.0e-6\n",
    "\n",
    "# Specify the maximum number of iterations\n",
    "max_iterations = 1000\n",
    "\n",
    "# Set the initial guess (pick any value here, or generate it at random)\n",
    "x = np.array([1.0,1.1,2.5])\n",
    "\n",
    "# Iteration counter\n",
    "it = 0\n",
    "\n",
    "# While not converged, repeat\n",
    "while(it==0 or np.sqrt(np.sum(grad**2))>tol and it<max_iterations):\n",
    "    # Evaluate the gradient\n",
    "    grad = grad_f2(x)\n",
    "    # Refine the initial guess\n",
    "    x = x - rho*grad\n",
    "    # Increase the iteration number\n",
    "    it += 1\n",
    "\n",
    "print('Value of the function after convergence: ' + str(f(x)))\n",
    "print('Value of x at the optimum: ' + str(x))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
