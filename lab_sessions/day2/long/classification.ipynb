{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification:  Logistic Regression\n",
    "\n",
    "In this lab, we will explore a commonly-used classification method named logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.special as spec\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import array as arr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, please make sure you can run ```ipython notebook``` in the ```day2/long/``` folder.\n",
    "\n",
    "Now copy ```mushrooms.data``` and ```mushrooms.txt``` to the ```data``` subfolder of ```day2/long/```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this command should work without error:\n",
    "import pandas as pd\n",
    "data_path = 'data/mushrooms.data'\n",
    "mushrooms = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define some toy data from the grade point average example from the talks (real data later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = arr([3.26, 3.10, 3.42, 3.51, 3.62, 4.00])\n",
    "N = len(xs)\n",
    "X = np.vstack((np.ones((1,N)), xs)).T\n",
    "Y = arr([0,0,1,0,1,1])\n",
    "print('X',X)\n",
    "print('Y',Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_ns = np.where(Y>0.5)\n",
    "neg_ns = np.where(Y<=0.5)\n",
    "plt.scatter(X[pos_ns,1],np.ones_like(pos_ns),marker='o')\n",
    "plt.scatter(X[neg_ns,1],np.zeros_like(neg_ns),marker='x')\n",
    "plt.xlim(xs.min()-0.2, xs.max()+0.2)\n",
    "plt.xlabel('GPA')\n",
    "plt.ylabel('Admission')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood function\n",
    "The log likelihood function of logistic regression is as follows:\n",
    "\n",
    "$\\mathcal{L}(\\beta) = \\sum_{n=1}^N \\log \\mathcal{B}(Y_n | g^{-1}({\\bf X},{\\bf \\beta}))$\n",
    "\n",
    "where $g^{-1}$ is the inverse link function $g^{-1}({\\bf X},{\\bf \\beta}) = \\sigma({\\bf X}^\\top {\\bf \\beta})$\n",
    "and $\\sigma$ is the softmax function $\\sigma(t) = \\frac{1}{1 + e^{-t}}$.\n",
    "\n",
    "If we maximize $\\mathcal{L}(\\beta)$ with respect to $\\beta$, then we have found the maximum likelihood solution that explains the data $({\\bf X},{\\bf Y})$. This allows one to make predictions for unseen $X$ and $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the inverse link function g^-1(eta)\n",
    "def invg(X_,beta_):\n",
    "    return spec.expit(np.dot(X_,beta_))\n",
    "\n",
    "#define the negative log likelihood:\n",
    "def neg_ll(beta_,X_,Y_): \n",
    "    mu = invg(X_,beta_)\n",
    "    return -stats.bernoulli.logpmf(Y_, mu).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent for Logistic Regression\n",
    "Gradient descent is a useful tool to maximize objective functions (more on that in this afternoon's session). To do that, we need to create a Python function that will give the derivative $\\frac{\\mathrm{d}}{\\mathrm{d} \\beta}\\mathcal{L}(\\beta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first derivative of negative log likelihood function for logistic regression:\n",
    "def deriv_neg_ll(beta_,X_,Y_):\n",
    "    (N_,_) = X_.shape\n",
    "    mu = invg(X_,beta_)\n",
    "    return -arr([X_[i]*(Y_[i] - mu[i]) for i in range(N_)]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Bonus Task)\n",
    "If you feel comfortable: try and derive the derivative (implemented above) for logistic regression:\n",
    "$\\frac{\\mathrm{d}}{\\mathrm{d} \\beta}\\mathcal{L}(\\beta) = \\sum_{n=1}^N X_n (Y_n - \\sigma({\\bf X_n}^\\top {\\bf \\beta}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "To perform gradient descent, we use the ```scipy.optimize``` package and call the ```fmin_bfgs``` function, giving it:\n",
    "\n",
    "1. the objective function;\n",
    "2. an initial guess of $\\beta$; and\n",
    "3. the derivative of the objective function. \n",
    "    \n",
    "```fmin_bfgs``` will find the minimum for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "beta0 = arr([-1.,1.]) #initial guess of coefficients\n",
    "beta_opt = opt.fmin_bfgs(lambda beta_: neg_ll(beta_,X,Y), beta0, fprime=lambda beta_: deriv_neg_ll(beta_,X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Print to the terminal 4 things:\n",
    "\n",
    "1. $\\beta_\\mathrm{opt}$;\n",
    "2. the log likelihood $\\mathcal{L}(\\beta_0)$ of the original guess for $\\beta$ that we used;\n",
    "3. the log likelihood $\\mathcal{L}(\\beta_\\mathrm{opt})$of the optimal $\\beta$ that was found; and\n",
    "4. the predictions that logistic regression makes for all the observed $(X_{1:N},Y_{1:N})$ defined in code earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "I will now plot the figure shown in the slides. Positive examples use the ```o``` marker, negative examples use the ```x``` marker and the probability of admission $p(Y_n = 1 | \\mu_{x})$ is the blue curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs1 = np.arange(xs.min()-0.2, xs.max()+0.2, 0.01)\n",
    "X1 = np.vstack((np.ones((1,len(xs1))),xs1)).T\n",
    "ys1 = invg(X1,beta_opt)\n",
    "\n",
    "plt.scatter(X[pos_ns,1],np.ones_like(pos_ns),marker='o')\n",
    "plt.scatter(X[neg_ns,1],np.zeros_like(neg_ns),marker='x')\n",
    "plt.plot(xs1, ys1)\n",
    "plt.xlim(xs1.min(), xs1.max())\n",
    "plt.xlabel('GPA')\n",
    "plt.ylabel('Admission')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Mushrooms\n",
    "Let's use a data set, provided by the University of California, Irvine (UCI). It consists of features of approximately 8,000 mushrooms and whether each mushroom was poisonous (```p```) or edible (```e```). \n",
    "\n",
    "We treat the prediction of whether a mushroom is edible (1) or poisonous (0) as a logistic regression. The independent variables will be the 22 features, all of which are *categorical*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mushrooms #browse data in a table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mushrooms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Reading the feature descriptors\n",
    "Take a few minutes to read the file ```mushrooms.txt``` (provided in the ```data/``` subfolder) to find out what the 22 features mean as best you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "What is the class occurrence of this data set, i.e., what percentage of the mushrooms are poisonous vs. edible?\n",
    "### Answer:\n",
    "[here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the data\n",
    "In its current state, the ```pandas``` dataframe ```mushrooms``` cannot be used with logistic regression because there are letters in the table instead of numbers. Pandas provides a convenient function, ```get_dummies``` to convert this table into a table of *one-hot* vectors.\n",
    "\n",
    "A *one-hot* vector represents a discrete feature $x_f$ that could take 1 of $V$ distinct values ($x_f = s_1 \\mathrm{\\;or\\;} s_2 \\mathrm{\\;or\\;} \\dots \\mathrm{\\;or\\;} s_V \\}$ as a vector of size $V$ consisting of all zeros except for one location $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_cols = ['f%i'%j for j in range(1,23)]\n",
    "mcat = pd.get_dummies(mushrooms, columns=['edibility']+feature_cols)\n",
    "mcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to extract np.array's from dataframe and add intercept column:\n",
    "def one_hot(df):\n",
    "    X_, Y_ = df.iloc[:,1:].values, df.iloc[:,0].values\n",
    "    (N_,D_) = X_.shape\n",
    "    #add intercept column:\n",
    "    c = np.ones((N_,1))\n",
    "    return np.hstack((c,X_)), Y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing data. [Covered more in evaluation session]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split the data into training and testing 50%-50%\n",
    "np.random.seed(0)\n",
    "train_mask = np.random.rand(len(mcat)) < 0.50\n",
    "mcat_train = mcat[train_mask]\n",
    "mcat_test = mcat[~train_mask]\n",
    "\n",
    "#mushroom data set is too easy: let's randomly flip some dependent variables to make it harder\n",
    "#(please ignore):\n",
    "def corrupt(Y_,p=0.3):\n",
    "    flips = np.random.binomial(1,p,size=len(Y_)) #please ignore\n",
    "    return (1-flips)*Y_ + flips*(1-Y_) #please ignore\n",
    "\n",
    "#convert training data to one-hot representation:\n",
    "X, Y_easy = one_hot(mcat_train)\n",
    "Y = corrupt(Y_easy)\n",
    "\n",
    "#convert testing data to one-hot representation:\n",
    "Xtest, Ytest_easy = one_hot(mcat_test)\n",
    "Ytest = corrupt(Ytest_easy)\n",
    "\n",
    "print('X',X)\n",
    "print('Y',Y)\n",
    "print('Xtest',Xtest)\n",
    "print('Ytest',Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running logistic regression on mushrooms\n",
    "Below we finally apply logistic regression to the mushroom data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "(N,D) = X.shape\n",
    "beta0 = np.random.normal(0,1,size=(D,)) #initial guess of coefficients\n",
    "print('init_ll',-neg_ll(beta0,X,Y))\n",
    "beta_opt = opt.fmin_bfgs(lambda beta_: neg_ll(beta_,X,Y), beta0, fprime=lambda beta_: deriv_neg_ll(beta_,X,Y))\n",
    "print('max_ll',-neg_ll(beta_opt,X,Y))\n",
    "print('pred_E_Y',spec.expit(np.dot(X,beta_opt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Calculate the held-out accuracy of logistic regression on the mushroom data set. What are the most influential features for identifying poisonous mushrooms? What are the least influential? (Hint: use the bar chart then cross-reference with ```mushrooms.txt``` help file, and don't forget about the fact that the first coefficient is the intercept value.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rmse(X_,Y_,beta_):\n",
    "    #your code here\n",
    "    return rmse\n",
    "\n",
    "def accuracy(X_,Y_,beta_):\n",
    "    #your code here\n",
    "    return accuracy\n",
    "    \n",
    "print('RMSE, accuracy on train (beta_init)',rmse(X,Y,beta0), accuracy(X,Y,beta0))\n",
    "print('RMSE, accuracy on train (beta_opt)',rmse(X,Y,beta_opt), accuracy(X,Y,beta_opt))\n",
    "print('RMSE, accuracy on test (beta_opt)',rmse(Xtest,Ytest,beta_opt), accuracy(Xtest,Ytest,beta_opt))\n",
    "\n",
    "#plot the trained coefficients:\n",
    "plt.bar(range(D), beta_opt)\n",
    "plt.xlabel('feature id')\n",
    "plt.ylabel('strength of coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
