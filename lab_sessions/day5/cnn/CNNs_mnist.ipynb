{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "mnist_dict = fetch_mldata(\"MNIST original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the data\n",
    "X_all = mnist_dict['data']\n",
    "Y_all = mnist_dict['target']\n",
    "print(\"Shape of X_all: \", X_all.shape)\n",
    "print(\"Shape of Y_all: \", Y_all.shape)\n",
    "\n",
    "# Get the number of classes (10) and the dimensionality of the input data (28*28)\n",
    "num_classes = len(np.unique(Y_all))\n",
    "num_pixels = X_all.shape[1]\n",
    "print(\"Number of classes: \", num_classes)\n",
    "print(\"Number of pixels: \", num_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into training/test/validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of observations in each group\n",
    "n_training_cases = 60000\n",
    "n_test_cases = 8000\n",
    "\n",
    "# Create a permutation vector\n",
    "perm = np.random.permutation(X_all.shape[0])\n",
    "\n",
    "# Split into training/test/validation\n",
    "X_train = X_all[perm[:n_training_cases], :].astype(np.float32)\n",
    "Y_train = Y_all[perm[:n_training_cases], None].astype(np.int32)\n",
    "\n",
    "X_test = X_all[perm[n_training_cases:n_training_cases+n_test_cases], :].astype(np.float32)\n",
    "Y_test = Y_all[perm[n_training_cases:n_training_cases+n_test_cases], None].astype(np.int32)\n",
    "\n",
    "X_val = X_all[perm[n_training_cases+n_test_cases:], :].astype(np.float32)\n",
    "Y_val = Y_all[perm[n_training_cases+n_test_cases:], None].astype(np.int32)\n",
    "\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of X_val: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, axis=1) == np.squeeze(labels)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auxiliary TensorFlow functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weight(shape):\n",
    "    return tf.Variable( tf.truncated_normal(shape, stddev=0.01) )\n",
    "\n",
    "def create_bias(shape):\n",
    "    return tf.Variable( tf.constant(0.1, shape=shape) )\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def next_minibatch(batch_size):\n",
    "    # Create a vector with batch_size random integers\n",
    "    perm = np.random.permutation(X_train.shape[0])\n",
    "    perm = perm[:batch_size]\n",
    "    # Generate the minibatch\n",
    "    X_batch = X_train[perm, :]\n",
    "    Y_batch = Y_train[perm, :]\n",
    "    # Return the images and the labels\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "def all_computations(X, W_0, b_0, W_2, b_2, W_4, b_4, W_5, b_5):\n",
    "    Z_0 = tf.nn.relu( conv2d(X, W_0) + b_0 )\n",
    "    Z_1 = max_pool_2x2(Z_0)\n",
    "    Z_2 = tf.nn.relu( conv2d(Z_1, W_2) + b_2)\n",
    "    Z_3 = max_pool_2x2(Z_2)\n",
    "    aux4 = tf.reshape( Z_3, [-1, 7*7*64] )\n",
    "    Z_4 = tf.nn.relu( tf.matmul(aux4, W_4) + b_4 )\n",
    "    x_logits = tf.matmul(Z_4, W_5) + b_5\n",
    "    return( x_logits )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weights_0(W_0):\n",
    "    plt.figure()\n",
    "    for j in range(32):\n",
    "        ax = plt.subplot(4,8,j+1)\n",
    "        ax.imshow(np.squeeze(W_0[:,:,:,j]), cmap=plt.cm.gray)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_correct(X, predictions, labels, n_images=5):\n",
    "    idxCorrect = (np.argmax(predictions, axis=1) == np.squeeze(labels))\n",
    "    idx = np.nonzero(idxCorrect)[0]\n",
    "    plt.figure()\n",
    "    for j in range(n_images):\n",
    "        n = idx[j]\n",
    "        prob_sorted = np.sort(predictions[n,:])\n",
    "        classes_sorted = np.argsort(predictions[n,:])\n",
    "        x_reshaped = X[n,:].reshape((28, 28))\n",
    "        str_title = ''\n",
    "        \n",
    "        for c in range(4):\n",
    "            str_title = str_title+'Digit #'+str(classes_sorted[-c-1])+', prob='+str(prob_sorted[-c-1])+'\\n'\n",
    "        \n",
    "        ax = plt.subplot(1,n_images,j+1)\n",
    "        ax.imshow(x_reshaped, cmap=plt.cm.gray)\n",
    "        plt.title(str_title, fontsize=7)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_incorrect(X, predictions, labels, n_images=5):\n",
    "    idxIncorrect = (np.argmax(predictions, axis=1) != np.squeeze(labels))\n",
    "    idx = np.nonzero(idxIncorrect)[0]\n",
    "    plt.figure()\n",
    "    for j in range(n_images):\n",
    "        n = idx[j]\n",
    "        prob_sorted = np.sort(predictions[n,:])\n",
    "        classes_sorted = np.argsort(predictions[n,:])\n",
    "        x_reshaped = X[n,:].reshape((28, 28))\n",
    "        str_title = ''\n",
    "        \n",
    "        for c in range(4):\n",
    "            str_title = str_title+'Digit #'+str(classes_sorted[-c-1])+', prob='+str(prob_sorted[-c-1])+'\\n'\n",
    "        \n",
    "        ax = plt.subplot(1,n_images,j+1)\n",
    "        ax.imshow(x_reshaped, cmap=plt.cm.gray)\n",
    "        plt.title(str_title, fontsize=7)\n",
    "        plt.axis('off')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow code** (computational graph + session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the computational graph\n",
    "learning_rate = 0.0001\n",
    "batch_size = 100\n",
    "max_iterations = 7000\n",
    "\n",
    "K_0 = 32\n",
    "K_1 = 64\n",
    "K_4 = 1024\n",
    "\n",
    "my_graph = tf.Graph()\n",
    "with my_graph.as_default():\n",
    "    # (a) Input data\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, num_pixels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    tf_test_data = tf.constant(X_test)\n",
    "    tf_val_data = tf.constant(X_val)\n",
    "    \n",
    "    # Reshape the images to make them (images x height x width x color)\n",
    "    train_images = tf.reshape(tf_train_data, [-1,28,28,1])\n",
    "    test_images = tf.reshape(tf_test_data, [-1,28,28,1])\n",
    "    val_images = tf.reshape(tf_val_data, [-1,28,28,1])\n",
    "    \n",
    "    # (b) Variables\n",
    "    W_0 = create_weight([5, 5, 1, K_0])\n",
    "    b_0 = create_bias([K_0])\n",
    "    W_2 = create_weight([5, 5, K_0, K_1])\n",
    "    b_2 = create_bias([K_1])\n",
    "    W_4 = create_weight([7 * 7 * K_1, K_4])\n",
    "    b_4 = create_bias([K_4])\n",
    "    W_5 = create_weight([K_4, num_classes])\n",
    "    b_5 = create_bias([num_classes])\n",
    "    \n",
    "    # (c) Computations\n",
    "    train_logits = all_computations(train_images, W_0, b_0, W_2, b_2, W_4, b_4, W_5, b_5)\n",
    "    loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(train_logits, tf_train_labels) )\n",
    "\n",
    "    # (d) Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # (e) Other tasks\n",
    "    train_prediction = tf.nn.softmax( train_logits )\n",
    "    val_prediction = tf.nn.softmax( all_computations(val_images, W_0, b_0, W_2, b_2, W_4, b_4, W_5, b_5) )\n",
    "    test_prediction = tf.nn.softmax( all_computations(test_images, W_0, b_0, W_2, b_2, W_4, b_4, W_5, b_5) )\n",
    "\n",
    "with tf.Session(graph=my_graph) as session:\n",
    "    # 1. We initialize the weights and biases. This is a one-time operation\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    # 2. Run SGD\n",
    "    for step in range(max_iterations):\n",
    "        # Get a new minibatch of data\n",
    "        X_batch, Y_batch = next_minibatch(batch_size)\n",
    "\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = { tf_train_data   : X_batch,\n",
    "                      tf_train_labels : np.squeeze(Y_batch) }\n",
    "\n",
    "        # Run the computations\n",
    "        _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict )\n",
    "\n",
    "        # Every 500 iterations\n",
    "        if (step % 500 == 0):\n",
    "            # Print the loss\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            # Obtain and print the accuracy on the training set\n",
    "            print(\" +Minibatch accuracy: %.1f%%\" % accuracy(predictions, Y_batch))\n",
    "            # Obtain and print the accuracy on the validation set\n",
    "            print(\" +Validation accuracy: %.1f%%\" % accuracy(val_prediction.eval(), Y_val))\n",
    "\n",
    "    # 3. Accuracty on the test set\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), Y_test))\n",
    "    \n",
    "    # 4. Plot correctly and incorrectly classified images\n",
    "    plot_incorrect(X_test, test_prediction.eval(), Y_test) \n",
    "    plot_correct(X_test, test_prediction.eval(), Y_test) \n",
    "    \n",
    "    # 5. Plot the weights of the first hidden layer\n",
    "    plot_weights_0(W_0.eval())\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
