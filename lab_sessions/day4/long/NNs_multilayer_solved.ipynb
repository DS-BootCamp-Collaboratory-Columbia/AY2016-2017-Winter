{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks in Tensorflow\n",
    "\n",
    "In this lab session, we will use the knowledge acquired in the tutorial to implement and train a deep neural network. In particular, we will:\n",
    "\n",
    "1. Implement a neural network with *one hidden layer* for multiclass classification using MNIST.\n",
    "\n",
    "2. Implement a neural network with *two hidden layers* for multiclass classification using MNIST.\n",
    "\n",
    "##Â Load the Data / Auxiliary Functions\n",
    "\n",
    "We copy here the auxiliary functions that we already used in the tutorial, including those to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_all:  (70000, 784)\n",
      "Shape of Y_all:  (70000,)\n",
      "Number of classes:  10\n",
      "Number of pixels:  784\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "mnist_dict = fetch_mldata(\"MNIST original\")\n",
    "\n",
    "# Get the data\n",
    "X_all = mnist_dict['data']\n",
    "Y_all = mnist_dict['target']\n",
    "print(\"Shape of X_all: \", X_all.shape)\n",
    "print(\"Shape of Y_all: \", Y_all.shape)\n",
    "\n",
    "# Get the number of classes (10) and the dimensionality of the input data (28*28)\n",
    "num_classes = len(np.unique(Y_all))\n",
    "num_pixels = X_all.shape[1]\n",
    "print(\"Number of classes: \", num_classes)\n",
    "print(\"Number of pixels: \", num_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (60000, 784) \t Shape of Y_train:  (60000, 1)\n",
      "Shape of X_test:  (8000, 784) \t Shape of Y_test:  (8000, 1)\n",
      "Shape of X_val:  (2000, 784) \t Shape of Y_val:  (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Number of observations in each group\n",
    "n_training_cases = 60000\n",
    "n_test_cases = 8000\n",
    "\n",
    "# Create a permutation vector to shuffle the images\n",
    "perm = np.random.permutation(X_all.shape[0])\n",
    "\n",
    "# Split into training/test/validation\n",
    "X_train = X_all[perm[:n_training_cases], :].astype(np.float32)\n",
    "Y_train = Y_all[perm[:n_training_cases], None].astype(np.int32)\n",
    "\n",
    "X_test = X_all[perm[n_training_cases:n_training_cases+n_test_cases], :].astype(np.float32)\n",
    "Y_test = Y_all[perm[n_training_cases:n_training_cases+n_test_cases], None].astype(np.int32)\n",
    "\n",
    "X_val = X_all[perm[n_training_cases+n_test_cases:], :].astype(np.float32)\n",
    "Y_val = Y_all[perm[n_training_cases+n_test_cases:], None].astype(np.int32)\n",
    "\n",
    "print(\"Shape of X_train: \", X_train.shape, \"\\t Shape of Y_train: \", Y_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape, \"\\t Shape of Y_test: \", Y_test.shape)\n",
    "print(\"Shape of X_val: \", X_val.shape, \"\\t Shape of Y_val: \", Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_small:  (60000, 784) \t Shape of Y_train_small:  (60000, 1)\n"
     ]
    }
   ],
   "source": [
    "# [In case you want to switch to a smaller dataset]\n",
    "train_subset = 60000   # <--- Replace with some other number if you wish to select a smaller dataset\n",
    "\n",
    "X_train_small = X_train[:train_subset, :]  # First images:\n",
    "Y_train_small = Y_train[:train_subset, :]  # We don't have to choose a random set because images have already been shuffled\n",
    "\n",
    "print(\"Shape of X_train_small: \", X_train_small.shape, \"\\t Shape of Y_train_small: \", Y_train_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # Return % of correctly classified images\n",
    "    return (100.0 * np.sum(np.argmax(predictions, axis=1) == np.squeeze(labels)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_minibatch(X_, Y_, batch_size):\n",
    "    # Create a vector with batch_size random integers\n",
    "    perm = np.random.permutation(X_.shape[0])\n",
    "    perm = perm[:batch_size]\n",
    "    # Generate the minibatch\n",
    "    X_batch = X_[perm, :]\n",
    "    Y_batch = Y_[perm, :]\n",
    "    # Return the images and the labels\n",
    "    return X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weight(shape):\n",
    "    # creates and initializes a weight matrix of the specified size\n",
    "    return tf.Variable( tf.truncated_normal(shape, stddev=0.01) )\n",
    "\n",
    "def create_bias(shape):\n",
    "    # creates and initializes a bias term of the specified size\n",
    "    return tf.Variable( tf.constant(0.1, shape=shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multinomial Logistic Regression (One Hidden Layer)\n",
    "\n",
    "Modify the code from the tutorial to implement a neural network with one hidden layer. In particular, we will consider the following model. Let $\\mathbf{X}$ be the matrix containing the images (with one row per image).\n",
    "\n",
    "**First layer (hidden layer).** The first layer of the NN computes the auxiliary variables $\\mathbf{Z}^{(0)}$ as\n",
    "$$\n",
    "\\mathbf{Z}^{(0)} = f\\left(\\mathbf{X}\\mathbf{W}^{(0)} + \\mathbf{b}^{(0)}\\right),\n",
    "$$\n",
    "where\n",
    "\n",
    "- $\\mathbf{W}^{(0)}$ denotes the weights of the top layer. The size of this matrix is $D\\times K$, where $D$ is the dimensionality of the inputs (i.e., the number of pixels of each image), and $K$ is the number of hidden units. You can use $K=2000$.\n",
    "\n",
    "- $\\mathbf{b}^{(0)}$ denotes the biases of the top layer (a vector of length $K$).\n",
    "\n",
    "- $f(\\cdot)$ is a non-linear function. For concreteness, we will use a rectified linear unit (ReLU), which is defined as\n",
    "$$\n",
    "f(x) = \\max(0,x)\n",
    "$$\n",
    "In Tensorflow, we can use the ReLU function `tf.nn.relu()`.\n",
    "\n",
    "*Note:* $\\mathbf{Z}^{(0)}$ is an auxiliary variable, but it is not a variable to be optimized. This means that it must *not* be declared in block b (Variables) of the computational graph. The only variables that must appear in block b (Variables) are the weights and biases of both layers.\n",
    "\n",
    "**Second layer (output layer).** The second layer of the NN is a softmax layer like the one implemented in the previous tutorial. The only difference is that the input to this layer is $\\mathbf{Z}^{(0)}$ instead of $\\mathbf{X}$. That is,\n",
    "$$\n",
    "p(y_n=j\\;|\\; \\mathbf{z}_n^{(0)}) = \\frac{e^{{\\mathbf{z}_n^{(0)}}\\mathbf{w}_j^{(1)} + b_j^{(1)}} }{\\sum_{j^\\prime=1}^J e^{{\\mathbf{z}_n^{(0)}}\\mathbf{w}_{j^\\prime}^{(1)} + b_{j^\\prime}^{(1)} }},\\qquad j=1,\\ldots,J\n",
    "$$\n",
    "Similarly to the tutorial, we need $J$ weight vectors $\\mathbf{w}_j^{(1)}$ (each of lengts $K$) and $J$ biases $b_j^{(1)}$, which we can group together in a $K\\times J$ matrix $\\mathbf{W}^{(1)}$ and a $J$-vector $\\mathbf{b}^{(1)}$. This is as shown in the tutorial; the only difference now is the size of $\\mathbf{W}^{(1)}$, which is now $K\\times J$ instead of $D\\times J$ to match the size of the input $\\mathbf{Z}^{(0)}$.\n",
    "\n",
    "**[Task]** Implement and train the network. Find the validation and test accuracy. Is it better or worse than in the tutorial?\n",
    "\n",
    "Some hints:\n",
    "\n",
    "1. Use 10000 iterations of SGD with minibatch size of 100. Use AdamOptimizer with learning rate $10^{-5}$.\n",
    "\n",
    "2. Think about how many latent variables there are and what are their sizes.\n",
    "\n",
    "3. You may use the following function that encapsulates the operations across the entire network:\n",
    "\n",
    "```python\n",
    "def all_nn_computations(X, weights_0, biases_0, weights_1, biases_1):\n",
    "    Z_0 = tf.nn.relu( tf.matmul(X, weights_0) + biases_0 )\n",
    "    logits = tf.matmul(Z_0, weights_1) + biases_1\n",
    "    return logits\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_nn_computations(X, weights_0, biases_0, weights_1, biases_1):\n",
    "    Z_0 = tf.nn.relu( tf.matmul(X, weights_0) + biases_0 )\n",
    "    logits = tf.matmul(Z_0, weights_1) + biases_1\n",
    "    return logits\n",
    "\n",
    "# Create the computational graph\n",
    "learning_rate = 1.0e-5\n",
    "batch_size = 100\n",
    "max_iterations = 10000\n",
    "K = 2000 # number of neurons in the hidden layer\n",
    "\n",
    "graph_MLR_2layers = tf.Graph()\n",
    "with graph_MLR_2layers.as_default():\n",
    "    # (a) Input data\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, num_pixels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    tf_test_data = tf.constant(X_test)\n",
    "    tf_val_data = tf.constant(X_val)\n",
    "\n",
    "    # (b) Variables\n",
    "    weights_0 = create_weight([num_pixels, K])\n",
    "    biases_0 = create_bias([K])\n",
    "    weights_1 = create_weight([K, num_classes])\n",
    "    biases_1 = create_bias([num_classes])\n",
    "\n",
    "    # (c) Computations\n",
    "    train_logits = all_nn_computations(tf_train_data, weights_0, biases_0, weights_1, biases_1)\n",
    "    loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(train_logits, tf_train_labels) )\n",
    "\n",
    "    # (d) Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # (e) Other tasks\n",
    "    train_prediction = tf.nn.softmax( train_logits )\n",
    "    val_prediction = tf.nn.softmax( all_nn_computations(tf_val_data, weights_0, biases_0, weights_1, biases_1) )\n",
    "    test_prediction = tf.nn.softmax( all_nn_computations(tf_test_data, weights_0, biases_0, weights_1, biases_1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 6.597075\n",
      " +Minibatch accuracy: 5.0%\n",
      " +Validation accuracy: 12.2%\n",
      "Minibatch loss at step 500: 0.265746\n",
      " +Minibatch accuracy: 92.0%\n",
      " +Validation accuracy: 91.4%\n",
      "Minibatch loss at step 1000: 0.114103\n",
      " +Minibatch accuracy: 96.0%\n",
      " +Validation accuracy: 94.1%\n",
      "Minibatch loss at step 1500: 0.110969\n",
      " +Minibatch accuracy: 97.0%\n",
      " +Validation accuracy: 95.0%\n",
      "Minibatch loss at step 2000: 0.036320\n",
      " +Minibatch accuracy: 98.0%\n",
      " +Validation accuracy: 96.0%\n",
      "Minibatch loss at step 2500: 0.116913\n",
      " +Minibatch accuracy: 96.0%\n",
      " +Validation accuracy: 96.5%\n",
      "Minibatch loss at step 3000: 0.085171\n",
      " +Minibatch accuracy: 98.0%\n",
      " +Validation accuracy: 97.2%\n",
      "Minibatch loss at step 3500: 0.052354\n",
      " +Minibatch accuracy: 99.0%\n",
      " +Validation accuracy: 97.0%\n",
      "Minibatch loss at step 4000: 0.017966\n",
      " +Minibatch accuracy: 99.0%\n",
      " +Validation accuracy: 97.0%\n",
      "Minibatch loss at step 4500: 0.033881\n",
      " +Minibatch accuracy: 99.0%\n",
      " +Validation accuracy: 97.2%\n",
      "Minibatch loss at step 5000: 0.009183\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 97.5%\n",
      "Minibatch loss at step 5500: 0.004808\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 97.2%\n",
      "Minibatch loss at step 6000: 0.010904\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 97.5%\n",
      "Minibatch loss at step 6500: 0.012984\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 97.7%\n",
      "Minibatch loss at step 7000: 0.009634\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 97.7%\n",
      "Minibatch loss at step 7500: 0.004324\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 97.6%\n",
      "Minibatch loss at step 8000: 0.011412\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 97.5%\n",
      "Minibatch loss at step 8500: 0.003758\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 97.8%\n",
      "Minibatch loss at step 9000: 0.002850\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 97.4%\n",
      "Minibatch loss at step 9500: 0.001770\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 97.8%\n",
      "Test accuracy: 97.5%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_MLR_2layers) as session:\n",
    "    # 1. We initialize the weights and biases. This is a one-time operation\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    # 2. Run SGD\n",
    "    for step in range(max_iterations):\n",
    "        # Get a new minibatch of data\n",
    "        X_batch, Y_batch = next_minibatch(X_train_small, Y_train_small, batch_size)\n",
    "\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch\n",
    "        feed_dict = { tf_train_data   : X_batch,\n",
    "                      tf_train_labels : np.squeeze(Y_batch) }\n",
    "\n",
    "        # Run the computations\n",
    "        _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict )\n",
    "\n",
    "        # Every 500 iterations\n",
    "        if (step % 500 == 0):\n",
    "            # Print the loss\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            # Obtain and print the accuracy on the training set\n",
    "            print(\" +Minibatch accuracy: %.1f%%\" % accuracy(predictions, Y_batch))\n",
    "            # Obtain and print the accuracy on the validation set\n",
    "            print(\" +Validation accuracy: %.1f%%\" % accuracy(val_prediction.eval(), Y_val))\n",
    "\n",
    "    # 3. Accuracty on the test set\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multinomial Logistic Regression (Two Hidden Layers)\n",
    "\n",
    "Now, extend the model with a second hidden layer. The resulting model is described below.\n",
    "\n",
    "**First layer (hidden layer).** We will use a standard linear layer with a ReLU output,\n",
    "$$\n",
    "\\mathbf{Z}^{(0)} = f\\left(\\mathbf{X}\\mathbf{W}^{(0)} + \\mathbf{b}^{(0)}\\right),\n",
    "$$\n",
    "where $f(\\cdot)$ is the ReLU function.\n",
    "\n",
    "**Second layer (hidden layer).** Again, we will use a standard linear layer with a ReLU output,\n",
    "$$\n",
    "\\mathbf{Z}^{(1)} = f\\left(\\mathbf{Z}^{(0)}\\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}\\right)\n",
    "$$\n",
    "\n",
    "**Third layer (output layer).** As before, we will use a softmax function,\n",
    "$$\n",
    "p(y_n=j\\;|\\; \\mathbf{z}_n^{(1)}) = \\frac{e^{{\\mathbf{z}_n^{(1)}}\\mathbf{w}_j^{(2)} + b_j^{(2)}} }{\\sum_{j^\\prime=1}^J e^{{\\mathbf{z}_n^{(1)}}\\mathbf{w}_{j^\\prime}^{(2)} + b_{j^\\prime}^{(2)} }},\\qquad j=1,\\ldots,J\n",
    "$$\n",
    "\n",
    "Now, the size of $\\mathbf{W}^{(0)}$ is $D\\times K_0$, the size of $\\mathbf{W}^{(1)}$ is $K_0\\times K_1$, and the size of $\\mathbf{W}^{(2)}$ is $K_1\\times J$. You may use $K_0=1000$ and $K_1=100$. The bias vectors $\\mathbf{b}^{(0)}$ and $\\mathbf{b}^{(1)}$ have length $K_0$ and $K_1$, respectively.\n",
    "\n",
    "**[Task]** Implement and train the network.\n",
    "\n",
    "Hints:\n",
    "\n",
    "1. Use SGD (15000 iterations, batch size of 100) and AdamOptimizer. Set the learning rate to $10^{-5}$.\n",
    "\n",
    "2. You need to re-define `all_nn_computations()`. Make sure you do that appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_nn_computations(X, weights_0, biases_0, weights_1, biases_1, weights_2, biases_2):\n",
    "    Z_0 = tf.nn.relu( tf.matmul(X, weights_0) + biases_0 )\n",
    "    Z_1 = tf.nn.relu( tf.matmul(Z_0, weights_1) + biases_1 )\n",
    "    logits = tf.matmul(Z_1, weights_2) + biases_2\n",
    "    return logits\n",
    "\n",
    "# Create the computational graph\n",
    "learning_rate = 1.0e-5\n",
    "batch_size = 100\n",
    "max_iterations = 15000\n",
    "K_0 = 1000 # number of neurons in each hidden layer\n",
    "K_1 = 100\n",
    "\n",
    "graph_MLR_3layers = tf.Graph()\n",
    "with graph_MLR_3layers.as_default():\n",
    "    # (a) Input data\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, num_pixels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    tf_test_data = tf.constant(X_test)\n",
    "    tf_val_data = tf.constant(X_val)\n",
    "\n",
    "    # (b) Variables\n",
    "    weights_0 = create_weight([num_pixels, K_0])\n",
    "    biases_0 = create_bias([K_0])\n",
    "    weights_1 = create_weight([K_0, K_1])\n",
    "    biases_1 = create_bias([K_1])\n",
    "    weights_2 = create_weight([K_1, num_classes])\n",
    "    biases_2 = create_bias([num_classes])\n",
    "\n",
    "    # (c) Computations\n",
    "    train_logits = all_nn_computations(tf_train_data, weights_0, biases_0, weights_1, biases_1, weights_2, biases_2)\n",
    "    loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(train_logits, tf_train_labels) )\n",
    "\n",
    "    # (d) Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # (e) Other tasks\n",
    "    train_prediction = tf.nn.softmax( train_logits )\n",
    "    val_prediction = tf.nn.softmax( all_nn_computations(tf_val_data, weights_0, biases_0, weights_1, biases_1, weights_2, biases_2) )\n",
    "    test_prediction = tf.nn.softmax( all_nn_computations(tf_test_data, weights_0, biases_0, weights_1, biases_1, weights_2, biases_2) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.336803\n",
      " +Minibatch accuracy: 10.0%\n",
      " +Validation accuracy: 6.8%\n",
      "Minibatch loss at step 1000: 0.321411\n",
      " +Minibatch accuracy: 91.0%\n",
      " +Validation accuracy: 93.3%\n",
      "Minibatch loss at step 2000: 0.091415\n",
      " +Minibatch accuracy: 98.0%\n",
      " +Validation accuracy: 95.0%\n",
      "Minibatch loss at step 3000: 0.085133\n",
      " +Minibatch accuracy: 99.0%\n",
      " +Validation accuracy: 96.0%\n",
      "Minibatch loss at step 4000: 0.066861\n",
      " +Minibatch accuracy: 99.0%\n",
      " +Validation accuracy: 96.7%\n",
      "Minibatch loss at step 5000: 0.068224\n",
      " +Minibatch accuracy: 97.0%\n",
      " +Validation accuracy: 97.0%\n",
      "Minibatch loss at step 6000: 0.066474\n",
      " +Minibatch accuracy: 98.0%\n",
      " +Validation accuracy: 97.9%\n",
      "Minibatch loss at step 7000: 0.032801\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 98.0%\n",
      "Minibatch loss at step 8000: 0.038483\n",
      " +Minibatch accuracy: 99.0%\n",
      " +Validation accuracy: 97.9%\n",
      "Minibatch loss at step 9000: 0.039843\n",
      " +Minibatch accuracy: 99.0%\n",
      " +Validation accuracy: 98.0%\n",
      "Minibatch loss at step 10000: 0.085233\n",
      " +Minibatch accuracy: 99.0%\n",
      " +Validation accuracy: 98.3%\n",
      "Minibatch loss at step 11000: 0.014024\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 98.5%\n",
      "Minibatch loss at step 12000: 0.011835\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 98.3%\n",
      "Minibatch loss at step 13000: 0.039700\n",
      " +Minibatch accuracy: 98.0%\n",
      " +Validation accuracy: 98.6%\n",
      "Minibatch loss at step 14000: 0.009301\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 98.3%\n",
      "Minibatch loss at step 15000: 0.009835\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 98.5%\n",
      "Minibatch loss at step 16000: 0.006917\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 98.3%\n",
      "Minibatch loss at step 17000: 0.018495\n",
      " +Minibatch accuracy: 99.0%\n",
      " +Validation accuracy: 98.5%\n",
      "Minibatch loss at step 18000: 0.006422\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 98.3%\n",
      "Minibatch loss at step 19000: 0.006165\n",
      " +Minibatch accuracy: 100.0%\n",
      " +Validation accuracy: 98.4%\n",
      "Test accuracy: 97.9%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_MLR_3layers) as session:\n",
    "    # 1. We initialize the weights and biases. This is a one-time operation\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    # 2. Run SGD\n",
    "    for step in range(max_iterations):\n",
    "        # Get a new minibatch of data\n",
    "        X_batch, Y_batch = next_minibatch(X_train_small, Y_train_small, batch_size)\n",
    "\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = { tf_train_data   : X_batch,\n",
    "                      tf_train_labels : np.squeeze(Y_batch) }\n",
    "\n",
    "        # Run the computations\n",
    "        _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict )\n",
    "\n",
    "        # Every 1000 iterations\n",
    "        if (step % 1000 == 0):\n",
    "            # Print the loss\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            # Obtain and print the accuracy on the training set\n",
    "            print(\" +Minibatch accuracy: %.1f%%\" % accuracy(predictions, Y_batch))\n",
    "            # Obtain and print the accuracy on the validation set\n",
    "            print(\" +Validation accuracy: %.1f%%\" % accuracy(val_prediction.eval(), Y_val))\n",
    "\n",
    "    # 3. Accuracty on the test set\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), Y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
