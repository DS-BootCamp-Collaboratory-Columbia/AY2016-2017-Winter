{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks on Tensorflow\n",
    "\n",
    "*The material for this lab session was inspired by [Google's Deep Learning course on Udacity](https://www.udacity.com/course/deep-learning--ud730).*\n",
    "\n",
    "**Introduction.** Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. Neural networks provide an information processing paradigm that is loosely inspired by the way biological nervous systems, such as the brain, process information. They are composed of a large number of highly interconnected processing elements (neurones) that work together to solve specific problems. \n",
    "\n",
    "**Goals.** In this tutorial, we will train a Multinomial Logistic Regressor with gradient descent.\n",
    "\n",
    "**Packages.** We will make use of [Tensorflow](https://www.tensorflow.org). We import the package with\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "```\n",
    "\n",
    "**Data.** We will use MNIST as an example. MNIST is widely used as a testbed for neural networks, and it is even still used in research publications. It contains 70000 images corresponding to handwritten digits, each belongs to one of ten possible classes (the classes are from digit #0 to digit #9). Each image is of size $28\\times 28$, i.e., $784$ pixels in total.\n",
    "\n",
    "Thus, in contrast to other lab sessions, here we will consider *multiclass classification*. That is, we will predict the probability of each of the 10 different classes (digits).\n",
    "\n",
    "## Loading and Preprocessing the Data\n",
    "\n",
    "MNIST is a so common that is has been included in [`scikit-learn`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_mldata.html#sklearn.datasets.fetch_mldata). Just use\n",
    "```python\n",
    "mnist_dict = fetch_mldata(\"MNIST original\")\n",
    "```\n",
    "to load it.\n",
    "\n",
    "**Import packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "mnist_dict = fetch_mldata(\"MNIST original\")\n",
    "\n",
    "# Get the data\n",
    "X_all = mnist_dict['data']\n",
    "Y_all = mnist_dict['target']\n",
    "print(\"Shape of X_all: \", X_all.shape)\n",
    "print(\"Shape of Y_all: \", Y_all.shape)\n",
    "\n",
    "# Get the number of classes (10) and the dimensionality of the input data (28*28)\n",
    "num_classes = len(np.unique(Y_all))\n",
    "num_pixels = X_all.shape[1]\n",
    "print(\"Number of classes: \", num_classes)\n",
    "print(\"Number of pixels: \", num_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot two of the images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose two indexes\n",
    "idx1 = 0       # <--- You can choose any index between 0 and 69999 (each corresponds to a different image)\n",
    "idx2 = 40000   # <--- You can choose any index between 0 and 69999 (each corresponds to a different image)\n",
    "\n",
    "# Plot the two images\n",
    "plt.figure()\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.imshow(np.reshape(X_all[idx1,:], (28,28)), cmap=plt.get_cmap('gray'))\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.imshow(np.reshape(X_all[idx2,:], (28,28)), cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into training/test/validation sets.** We will split the data into a training set of 60000 images, a test set of 8000 images, and a validation set of 2000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of observations in each group\n",
    "n_training_cases = 60000\n",
    "n_test_cases = 8000\n",
    "\n",
    "# Create a permutation vector to shuffle the images\n",
    "perm = np.random.permutation(X_all.shape[0])\n",
    "\n",
    "# Split into training/test/validation\n",
    "X_train = X_all[perm[:n_training_cases], :].astype(np.float32)\n",
    "Y_train = Y_all[perm[:n_training_cases], None].astype(np.int32)\n",
    "\n",
    "X_test = X_all[perm[n_training_cases:n_training_cases+n_test_cases], :].astype(np.float32)\n",
    "Y_test = Y_all[perm[n_training_cases:n_training_cases+n_test_cases], None].astype(np.int32)\n",
    "\n",
    "X_val = X_all[perm[n_training_cases+n_test_cases:], :].astype(np.float32)\n",
    "Y_val = Y_all[perm[n_training_cases+n_test_cases:], None].astype(np.int32)\n",
    "\n",
    "print(\"Shape of X_train: \", X_train.shape, \"\\t Shape of Y_train: \", Y_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape, \"\\t Shape of Y_test: \", Y_test.shape)\n",
    "print(\"Shape of X_val: \", X_val.shape, \"\\t Shape of Y_val: \", Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Also create a smaller dataset.** We will not apply any special preprocessing to the data. However, we will initially consider a smaller subset of the training set of $1000$ observations only, in order to speed-up the computations. (We will revert back to the $60000$ training instances later, when we consider stochastic gradient ascent.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For now, we take a subset of the training data to speed-up the computation\n",
    "train_subset = 1000   # number of observations to keep\n",
    "\n",
    "X_train_small = X_train[:train_subset, :]  # First 1000 images:\n",
    "Y_train_small = Y_train[:train_subset, :]  # We don't have to choose a random set because images have already been shuffled\n",
    "\n",
    "print(\"Shape of X_train_small: \", X_train_small.shape, \"\\t Shape of Y_train_small: \", Y_train_small.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression (MLR) with Tensorflow\n",
    "\n",
    "**Introduction.** We will implement a neural network without hidden layers. This is a common model more commonly known as multinomial logistic regression. It is a multiclass generalization of logistic regression. It also has weights and biases. However, we have one weight vector $w_j$ and one bias $b_j$ for each class $j$. While in logistic regression we use the sigmoid function to model the probability of the outputs, in multinomial logistic regression we use the softmax function. Thus, the probability (likelihood) of the $j$-th label for datapoint $n$ is given by\n",
    "$$\n",
    "p(y_n=j\\;|\\; x_n, \\{w_j, b_j\\})=\\frac{\\text{e}^{(w^\\top_j x_n + b_j)}}{\\sum_{j^\\prime=1}^{J} \\text{e}^{(w^\\top_{j^\\prime} x_n + b_{j^\\prime})}},\\qquad j=1,\\ldots,J\n",
    "$$\n",
    "The softmax function converts a vector of real numbers into a probability vector. In this case, these real numbers are the \"strength\" of the prediction for each class. For instance, if $w^\\top_1 x_n + b_1$ is very large, then class $1$ will be the most likely. If $w^\\top_1 x_n + b_1$ and $w^\\top_2 x_n + b_2$ are equal but they are much larger than the rest, then classes 1 and 2 will be equally likely (with probabilities close to $0.5$ each), while the rest of the classes will have probabilities very close to zero. Tensorflow incorporates a `softmax` function that passes its argument through the softmax function in a numerically stable manner and returns a set of probabilities (one for each class).\n",
    "\n",
    "The goal is to find the weights $w_j$ and biases $b_j$ such that the probability of the observed outcomes in the training set are maximized. As you know, this corresponds to *maximum likelihood (ML) estimation*.\n",
    "\n",
    "**Accuracy.** We define a very simple auxiliary function to compute the accuracy, which we define as the *percentage of correctly classified images*. The auxiliary function takes the predictions as input (either the arguments of the softmax of the ouptut of the softmax) and the observed labels, and it simply computes the average number of correctly classified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # Return % of correctly classified images\n",
    "    return (100.0 * np.sum(np.argmax(predictions, axis=1) == np.squeeze(labels)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Computational Graph\n",
    "\n",
    "**Introduction.** In TensorFlow, you first describe the computation that you would like to see performed. That is, you describe the inputs, the variables, and the operations. In this description, no actual computations is performed. This is like writing down the model on a piece of paper: you just specify which model you are using, but you do not compute anything yet. In Tensorflow, this is known as the \"computational graph.\" Each variable is a \"node\" on this graph, and the edges of the computational graph connect nodes that interact together in some operation.\n",
    "\n",
    "For now, all we need to know is that the definition of the computational graph appears under the block starting with\n",
    "```python\n",
    "with graph_name.as_default():\n",
    "```\n",
    "Within this block, we will call the function `all_nn_computations()` that we define next. This function simply takes the weights and biases and performs the operation $x_n^\\top w_j + b_j$ for each class $j$.\n",
    "\n",
    "The computational graph consists on all steps (a)-(e), which we cover below.\n",
    "\n",
    "**(a) Input data.** As input, we need: the training images and their labels, the validation images, and the test images. `tf.constant` creates a variable that will not change during optimization.\n",
    "\n",
    "```python\n",
    "    # (a) Input data\n",
    "    #     Load the training, validation and test data into constants that are\n",
    "    #     attached to the graph\n",
    "    tf_train_data = tf.constant(X_train_small)  # use the small dataset\n",
    "    tf_train_labels = tf.constant(np.squeeze(Y_train_small))\n",
    "    tf_test_data = tf.constant(X_test)\n",
    "    tf_val_data = tf.constant(X_val)\n",
    "```\n",
    "\n",
    "**(b) Variables.** The variables are the weights $w_j$, which we group in a $D\\times J$ matrix (number of pixels $\\times$ number of classes); and the biases $b_j$, which we group in a $J$-dimensional vector. Since we will do gradient descent, we need an initial guess for these variables. We define auxiliary functions to create and initialize the weights and biases. We follow the typical approach in NNs to initialize the variables\n",
    "\n",
    "- Initialize the weights randomly (using a truncated Gaussian distribution with small standard deviation).\n",
    "\n",
    "- Initialize the biases to some small positive constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weight(shape):\n",
    "    # creates and initializes a weight matrix of the specified size\n",
    "    return tf.Variable( tf.truncated_normal(shape, stddev=0.01) )\n",
    "\n",
    "def create_bias(shape):\n",
    "    # creates and initializes a bias term of the specified size\n",
    "    return tf.Variable( tf.constant(0.1, shape=shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, block (b) becomes\n",
    "\n",
    "```python\n",
    "    # (b) Variables\n",
    "    #     Indicate the parameters that we need to infer\n",
    "    weights = create_weight( [num_pixels, num_classes] )\n",
    "    biases = create_bias( [num_classes] )\n",
    "```\n",
    "\n",
    "**(c) Computations.** The computations of the neural network with no hidden layers are simple: simply multiply the inputs with the weights and add the biases. (We should additionally pass the result through the softmax function afterwards, but we will do this last computation later in the code). We create a function that does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_nn_computations(X, weights, biases):\n",
    "    return tf.matmul(X, weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compute the **loss function** here. We need the log-probabilities of the *observed* class for all the observations in our dataset, i.e., we need $\\hat{y}_n$. We need to sum (or average) these log-probabilities for all data points. These operations are so common that TensorFlow has a function for that, called `sparse_softmax_cross_entropy_with_logits`. This function returns the log-probabilities (under the softmax) of the *observed* classes. In other words, this produces\n",
    "$$\\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N \\log p(\\hat{y}_n\\;|\\; W, b)$$\n",
    "(TensorFlow gives a negative log-likelihood and then *minimizes* the objective.)\n",
    "\n",
    "\n",
    "\n",
    "Thus, block (c) becomes:\n",
    "```python\n",
    "    # (c) Computations\n",
    "    #     Indicate the computations that we want to perform with the variables and data\n",
    "    train_logits = all_nn_computations(tf_train_data, weights, biases)\n",
    "    loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(train_logits, tf_train_labels) )\n",
    "```\n",
    "\n",
    "** (d) Optimization.** We need to indicate that we want to minimize the loss using gradient descent. The optimizer requires that we specify the step size (also called learning rate). Importantly, **we do not need to provide the gradient**. TensorFlow will compute the gradients for us. We don't need to do anything else. This is called *automatic differentiation*.\n",
    "```python\n",
    "    # (d) Optimizer\n",
    "    #     Indicate the optimization procedure that we want to use\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "```\n",
    "\n",
    "** (e) Other tasks.** You may want to create other variables for debugging purposes, or to obtain the performance on test/validation. Here, we obtain the predictions (the output of the softmax) for the training, testing, and validation sets. This will be useful later to print the performance during optimization.\n",
    "```python\n",
    "    # (e) Other tasks\n",
    "    #     Compute predictions on training, test, and validation\n",
    "    train_prediction = tf.nn.softmax( train_logits )\n",
    "    val_prediction = tf.nn.softmax( all_nn_computations(tf_val_data, weights, biases) )\n",
    "    test_prediction = tf.nn.softmax( all_nn_computations(tf_test_data, weights, biases) ) \n",
    "```\n",
    "\n",
    "**All together.** Putting all together, we get to the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Tensorflow graph for multinomial logistic regression\n",
    "graph_MLR = tf.Graph()\n",
    "\n",
    "with graph_MLR.as_default():\n",
    "    # (a) Input data\n",
    "    #     Load the training, validation and test data into constants that are\n",
    "    #     attached to the graph\n",
    "    tf_train_data = tf.constant(X_train_small)  # use the small dataset\n",
    "    tf_train_labels = tf.constant(np.squeeze(Y_train_small))\n",
    "    tf_test_data = tf.constant(X_test)\n",
    "    tf_val_data = tf.constant(X_val)\n",
    "    \n",
    "    # (b) Variables\n",
    "    #     Indicate the parameters that we need to infer\n",
    "    weights = create_weight( [num_pixels, num_classes] )\n",
    "    biases = create_bias( [num_classes] )\n",
    "    \n",
    "    # (c) Computations\n",
    "    #     Indicate the computations that we want to perform with the variables and data\n",
    "    train_logits = all_nn_computations(tf_train_data, weights, biases)\n",
    "    loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(train_logits, tf_train_labels) )\n",
    "    \n",
    "    # (d) Optimizer\n",
    "    #     Indicate the optimization procedure that we want to use\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "    \n",
    "    # (e) Other tasks\n",
    "    #     Compute predictions on training, test, and validation\n",
    "    train_prediction = tf.nn.softmax( train_logits )\n",
    "    val_prediction = tf.nn.softmax( all_nn_computations(tf_val_data, weights, biases) )\n",
    "    test_prediction = tf.nn.softmax( all_nn_computations(tf_test_data, weights, biases) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TensorFlow Session\n",
    "\n",
    "**Introduction.** The code above specifies the computational graph. It does *not* compute anything. It only specifies the way in which variables interact with each other.\n",
    "\n",
    "If we want to actually perform the optimization, we need to call the `run` method. Each time we call `run`, Tensorflow will execute the operations in the graph. The runtime operations are indicated in the block starting with\n",
    "```python\n",
    "with tf.Session(graph) as session:\n",
    "```\n",
    "\n",
    "**Gradient descent.** Now, we will run 500 gradient descent steps using TensorFlow. Recall that for gradient descent, we need the gradient of the objective function. As we mentioned above, the greatest advantage of TensorFlow is that it automatically computes all the derivatives for us. We do not need to do the math ourselves.\n",
    "\n",
    "*Note:* As convergence criterion, we will stop after 500 iterations. TensorFlow also allows more complicated stopping criteria, like stopping when the magnitude of the gradient is small. We will not do that here for simplicity.\n",
    "\n",
    "**Plotting function.** We use an auxiliary function for plotting the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weights(weights):\n",
    "    plt.figure()\n",
    "    for j in range(num_classes):\n",
    "        # Create and choose subplot\n",
    "        ax = plt.subplot(1,num_classes,j+1)\n",
    "        # Obtain the weights corresponding to class j\n",
    "        weights_j = weights[:,j]\n",
    "        # Reshape\n",
    "        weights_reshaped = np.reshape(weights_j,(28, 28))\n",
    "        # Plot\n",
    "        ax.imshow(weights_reshaped, cmap=plt.get_cmap('Greys'))\n",
    "        plt.axis('off')\n",
    "        plt.title('digit #'+str(j), fontsize=7.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps in the code.** In the code below, we\n",
    "\n",
    "1. Initialize the variables (weights and biases) using `initialize_all_variables()`. This might give a warning, depending on your TensorFlow version.\n",
    "\n",
    "2. Write a `for` loop for the gradient descent algorithm. At each step, we call `session.run()` to perform one optimization step. Every 100 iterations, we compute the loss function and the accuracy on the training and validation sets.\n",
    "\n",
    "3. Compute the performance on the test set.\n",
    "\n",
    "4. Plot the weight matrices for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iterations = 500\n",
    "\n",
    "with tf.Session(graph=graph_MLR) as session:\n",
    "    # 1. Initialize the weights and biases. This is a one-time operation\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    # 2. Run iterations of gradient descent\n",
    "    for step in range(max_iterations):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        \n",
    "        # Report every 100 iterations\n",
    "        if (step % 100 == 0):\n",
    "            # Print the loss\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            # Obtain and print the accuracy on the training set\n",
    "            print('  +Training accuracy: %.1f%%' % accuracy(predictions, Y_train[:train_subset, :]))\n",
    "            # Obtain and print the accuracy on the validation set\n",
    "            print('  +Validation accuracy: %.1f%%' % accuracy(val_prediction.eval(), Y_val))\n",
    "            \n",
    "    # 3. Accuracty on the test set\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), Y_test))\n",
    "    \n",
    "    # 4. Plot the weights\n",
    "    plot_weights(weights.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Questions]** Run the code above.\n",
    "\n",
    "1. What happens to the training accuracy? Does it differ from the test accuracy? Why?\n",
    "\n",
    "2. Look at the resulting plots of the weights. What do they represent? Why do they look like actual digits?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
