{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks on Tensorflow\n",
    "\n",
    "*The material for this lab session was inspired by [Google's Deep Learning course on Udacity](https://www.udacity.com/course/deep-learning--ud730).*\n",
    "\n",
    "**Introduction.** This extends the previous tutorial to perform stochastic gradient descent. Most of the steps are identicals, so we will avoid repetition and will include <font color=\"red\">comments only on those parts that are new</font>, which are marked in red.\n",
    "\n",
    "**Goals.** In this tutorial, we will train a Multinomial Logistic Regressor with stochastic gradient descent.\n",
    "\n",
    "**Import packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "mnist_dict = fetch_mldata(\"MNIST original\")\n",
    "\n",
    "# Get the data\n",
    "X_all = mnist_dict['data']\n",
    "Y_all = mnist_dict['target']\n",
    "print(\"Shape of X_all: \", X_all.shape)\n",
    "print(\"Shape of Y_all: \", Y_all.shape)\n",
    "\n",
    "# Get the number of classes (10) and the dimensionality of the input data (28*28)\n",
    "num_classes = len(np.unique(Y_all))\n",
    "num_pixels = X_all.shape[1]\n",
    "print(\"Number of classes: \", num_classes)\n",
    "print(\"Number of pixels: \", num_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into training/test/validation sets.** We will split the data into a training set of 60000 images, a test set of 8000 images, and a validation set of 2000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of observations in each group\n",
    "n_training_cases = 60000\n",
    "n_test_cases = 8000\n",
    "\n",
    "# Create a permutation vector to shuffle the images\n",
    "perm = np.random.permutation(X_all.shape[0])\n",
    "\n",
    "# Split into training/test/validation\n",
    "X_train = X_all[perm[:n_training_cases], :].astype(np.float32)\n",
    "Y_train = Y_all[perm[:n_training_cases], None].astype(np.int32)\n",
    "\n",
    "X_test = X_all[perm[n_training_cases:n_training_cases+n_test_cases], :].astype(np.float32)\n",
    "Y_test = Y_all[perm[n_training_cases:n_training_cases+n_test_cases], None].astype(np.int32)\n",
    "\n",
    "X_val = X_all[perm[n_training_cases+n_test_cases:], :].astype(np.float32)\n",
    "Y_val = Y_all[perm[n_training_cases+n_test_cases:], None].astype(np.int32)\n",
    "\n",
    "print(\"Shape of X_train: \", X_train.shape, \"\\t Shape of Y_train: \", Y_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape, \"\\t Shape of Y_test: \", Y_test.shape)\n",
    "print(\"Shape of X_val: \", X_val.shape, \"\\t Shape of Y_val: \", Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>[Only if your computer is slow]</font> Create a smaller dataset.** You can choose the number of images to keep. We will first try with the full $60000$ training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For now, we take a subset of the training data to speed-up the computation\n",
    "train_subset = 60000   # <--- We are using the full dataset. You can use a smaller number of images\n",
    "\n",
    "X_train_small = X_train[:train_subset, :]  #Â First \"train_subset\" images:\n",
    "Y_train_small = Y_train[:train_subset, :]  # We don't have to choose a random set because images have already been shuffled\n",
    "\n",
    "print(\"Shape of X_train_small: \", X_train_small.shape, \"\\t Shape of Y_train_small: \", Y_train_small.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Computational Graph\n",
    "\n",
    "**Accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # Return % of correctly classified images\n",
    "    return (100.0 * np.sum(np.argmax(predictions, axis=1) == np.squeeze(labels)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**(a) Input data.**</font> In stochastic gradient descent, we choose a new minibatch of data at each iteration. Thus, the dataset effectively \"changes\" between iterations. For that reason, we need *placeholders*: we do not know the value of the minibatch of training data at this point. Roughly, a placeholder says: \"right now I can't tell you the value of this variable, but I can guarantee that I will tell you the value whenever you need it later.\"\n",
    "```python\n",
    "    # (a) Input data\n",
    "    #     Load the training, validation and test data into constants that are\n",
    "    #     attached to the graph\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, num_pixels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    tf_test_data = tf.constant(X_test)\n",
    "    tf_val_data = tf.constant(X_val)\n",
    "```\n",
    "\n",
    "**(b) Variables.** This block doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weight(shape):\n",
    "    # creates and initializes a weight matrix of the specified size\n",
    "    return tf.Variable( tf.truncated_normal(shape, stddev=0.01) )\n",
    "\n",
    "def create_bias(shape):\n",
    "    # creates and initializes a bias term of the specified size\n",
    "    return tf.Variable( tf.constant(0.1, shape=shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # (b) Variables\n",
    "    #     Indicate the parameters that we need to infer\n",
    "    weights = create_weight( [num_pixels, num_classes] )\n",
    "    biases = create_bias( [num_classes] )\n",
    "```\n",
    "\n",
    "**(c) Computations.** This doesn't change. (Well, it changes only conceptually, because we obtain the loss as if we had a dataset with less observations.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_nn_computations(X, weights, biases):\n",
    "    return tf.matmul(X, weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # (c) Computations\n",
    "    #     Indicate the computations that we want to perform with the variables and data\n",
    "    train_logits = all_nn_computations(tf_train_data, weights, biases)\n",
    "    loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(train_logits, tf_train_labels) )\n",
    "```\n",
    "\n",
    "** <font color=\"red\">(d) Optimization.</font>** We are going to find the minimum of this loss using SGD. In SGD, we need to decrease the step size at each iteration to ensure convergence. We do that automatically using a method such as RMSProp or Adam. You don't need to know any further details about what these methods do; just keep in mind that they decrease the step size at each iteration of SGD. We typically use a very small learning rate (in this case, $10^{-5}$). Here, we will use the Adam method.\n",
    "```python\n",
    "    # (d) Optimizer\n",
    "    #     Indicate the optimization procedure that we want to use\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1.0e-5).minimize(loss)\n",
    "```\n",
    "\n",
    "** (e) Other tasks.** This doesn't change.\n",
    "```python\n",
    "    # (e) Other tasks\n",
    "    #     Compute predictions on training, test, and validation\n",
    "    train_prediction = tf.nn.softmax( train_logits )\n",
    "    val_prediction = tf.nn.softmax( all_nn_computations(tf_val_data, weights, biases) )\n",
    "    test_prediction = tf.nn.softmax( all_nn_computations(tf_test_data, weights, biases) ) \n",
    "```\n",
    "\n",
    "**All together.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the minibatch size (i.e., the number of datapoints that we will\n",
    "# use at each iteration of gradient descent)\n",
    "batch_size = 200\n",
    "\n",
    "# Create the computational graph\n",
    "graph_MLR_SGD = tf.Graph()\n",
    "\n",
    "with graph_MLR_SGD.as_default():\n",
    "    # (a) Input data\n",
    "    #     Load the training, validation and test data into constants that are\n",
    "    #     attached to the graph\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, num_pixels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    tf_test_data = tf.constant(X_test)\n",
    "    tf_val_data = tf.constant(X_val)\n",
    "    \n",
    "    # (b) Variables\n",
    "    #     Indicate the parameters that we need to infer\n",
    "    weights = create_weight( [num_pixels, num_classes] )\n",
    "    biases = create_bias( [num_classes] )\n",
    "    \n",
    "    # (c) Computations\n",
    "    #     Indicate the computations that we want to perform with the variables and data\n",
    "    train_logits = all_nn_computations(tf_train_data, weights, biases)\n",
    "    loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(train_logits, tf_train_labels) )\n",
    "    \n",
    "    # (d) Optimizer\n",
    "    #     Indicate the optimization procedure that we want to use\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1.0e-5).minimize(loss)\n",
    "    \n",
    "    # (e) Other tasks\n",
    "    #     Compute predictions on training, test, and validation\n",
    "    train_prediction = tf.nn.softmax( train_logits )\n",
    "    val_prediction = tf.nn.softmax( all_nn_computations(tf_val_data, weights, biases) )\n",
    "    test_prediction = tf.nn.softmax( all_nn_computations(tf_test_data, weights, biases) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The Session\n",
    "\n",
    "**<font color=\"red\">Stochastic gradient descent.</font>** We will run 20000 SGD steps.\n",
    "\n",
    "**<font color=\"red\">Minibatches of data.</font>** At each iteration of SGD, we need to subsample a new minibatch of data. We do that using an auxiliary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_minibatch(X_, Y_, batch_size):\n",
    "    # Create a vector with batch_size random integers\n",
    "    perm = np.random.permutation(X_.shape[0])\n",
    "    perm = perm[:batch_size]\n",
    "    # Generate the minibatch\n",
    "    X_batch = X_[perm, :]\n",
    "    Y_batch = Y_[perm, :]\n",
    "    # Return the images and the labels\n",
    "    return X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weights(weights):\n",
    "    plt.figure()\n",
    "    for j in range(num_classes):\n",
    "        # Create and choose subplot\n",
    "        ax = plt.subplot(1,num_classes,j+1)\n",
    "        # Obtain the weights corresponding to class j\n",
    "        weights_j = weights[:,j]\n",
    "        # Reshape\n",
    "        weights_reshaped = np.reshape(weights_j,(28, 28))\n",
    "        # Plot\n",
    "        ax.imshow(weights_reshaped, cmap=plt.get_cmap('Greys'))\n",
    "        plt.axis('off')\n",
    "        plt.title('digit #'+str(j), fontsize=7.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Steps in the code.</font>** The only differences now are that:\n",
    "\n",
    "- At each iteration of SGD, we need to obtain a new minibatch of data.\n",
    "\n",
    "- This minibatch is passed to the placeholders through the `feed_dict` parameter of the `run()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iterations = 20000\n",
    "\n",
    "with tf.Session(graph=graph_MLR_SGD) as session:\n",
    "\n",
    "    # 1. Initialize the weights and biases. This is a one-time operation\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    # 2. Run iterations of SGD\n",
    "    for step in range(max_iterations):\n",
    "        \n",
    "        # Pick a subset of the datapoints at random\n",
    "        X_batch, Y_batch = next_minibatch(X_train_small, Y_train_small, batch_size)\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = { tf_train_data   : X_batch,\n",
    "                      tf_train_labels : np.squeeze(Y_batch)}\n",
    "        \n",
    "        # Run the computations\n",
    "        _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict )\n",
    "        \n",
    "        # Report every 500 iterations\n",
    "        if (step % 500 == 0):\n",
    "            # Print the loss\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            # Obtain and print the accuracy on the training set\n",
    "            print(\" +Minibatch accuracy: %.1f%%\" % accuracy(predictions, Y_batch))\n",
    "            # Obtain and print the accuracy on the validation set\n",
    "            print(\" +Validation accuracy: %.1f%%\" % accuracy(val_prediction.eval(), Y_val))\n",
    "\n",
    "    # 3. Accuracty on the test set\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**<font color=\"red\">[Questions]</font>** Run the code above.\n",
    "\n",
    "1. Is the resulting accuracy on the test set better than it was in the previous tutorial? Why/Why not?\n",
    "\n",
    "2. What line(s) of code would you need to do in the code if you wanted to regularize the unobserved parameters?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
