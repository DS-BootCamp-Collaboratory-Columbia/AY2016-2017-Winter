{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks in Tensorflow\n",
    "\n",
    "In this lab session, we will use the knowledge acquired in the tutorial to implement and train a deep neural network. In particular, we will:\n",
    "\n",
    "1. Implement a neural network with *one hidden layer* for multiclass classification using MNIST.\n",
    "\n",
    "2. Implement a neural network with *two hidden layers* for multiclass classification using MNIST.\n",
    "\n",
    "##Â Load the Data / Auxiliary Functions\n",
    "\n",
    "We copy here the auxiliary functions that we already used in the tutorial, including those to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "mnist_dict = fetch_mldata(\"MNIST original\")\n",
    "\n",
    "# Get the data\n",
    "X_all = mnist_dict['data']\n",
    "Y_all = mnist_dict['target']\n",
    "print(\"Shape of X_all: \", X_all.shape)\n",
    "print(\"Shape of Y_all: \", Y_all.shape)\n",
    "\n",
    "# Get the number of classes (10) and the dimensionality of the input data (28*28)\n",
    "num_classes = len(np.unique(Y_all))\n",
    "num_pixels = X_all.shape[1]\n",
    "print(\"Number of classes: \", num_classes)\n",
    "print(\"Number of pixels: \", num_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of observations in each group\n",
    "n_training_cases = 60000\n",
    "n_test_cases = 8000\n",
    "\n",
    "# Create a permutation vector to shuffle the images\n",
    "perm = np.random.permutation(X_all.shape[0])\n",
    "\n",
    "# Split into training/test/validation\n",
    "X_train = X_all[perm[:n_training_cases], :].astype(np.float32)\n",
    "Y_train = Y_all[perm[:n_training_cases], None].astype(np.int32)\n",
    "\n",
    "X_test = X_all[perm[n_training_cases:n_training_cases+n_test_cases], :].astype(np.float32)\n",
    "Y_test = Y_all[perm[n_training_cases:n_training_cases+n_test_cases], None].astype(np.int32)\n",
    "\n",
    "X_val = X_all[perm[n_training_cases+n_test_cases:], :].astype(np.float32)\n",
    "Y_val = Y_all[perm[n_training_cases+n_test_cases:], None].astype(np.int32)\n",
    "\n",
    "print(\"Shape of X_train: \", X_train.shape, \"\\t Shape of Y_train: \", Y_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape, \"\\t Shape of Y_test: \", Y_test.shape)\n",
    "print(\"Shape of X_val: \", X_val.shape, \"\\t Shape of Y_val: \", Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [In case you want to switch to a smaller dataset]\n",
    "train_subset = 60000   # <--- Replace with some other number if you wish to select a smaller dataset\n",
    "\n",
    "X_train_small = X_train[:train_subset, :]  # First images:\n",
    "Y_train_small = Y_train[:train_subset, :]  # We don't have to choose a random set because images have already been shuffled\n",
    "\n",
    "print(\"Shape of X_train_small: \", X_train_small.shape, \"\\t Shape of Y_train_small: \", Y_train_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # Return % of correctly classified images\n",
    "    return (100.0 * np.sum(np.argmax(predictions, axis=1) == np.squeeze(labels)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_minibatch(X_, Y_, batch_size):\n",
    "    # Create a vector with batch_size random integers\n",
    "    perm = np.random.permutation(X_.shape[0])\n",
    "    perm = perm[:batch_size]\n",
    "    # Generate the minibatch\n",
    "    X_batch = X_[perm, :]\n",
    "    Y_batch = Y_[perm, :]\n",
    "    # Return the images and the labels\n",
    "    return X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weight(shape):\n",
    "    # creates and initializes a weight matrix of the specified size\n",
    "    return tf.Variable( tf.truncated_normal(shape, stddev=0.01) )\n",
    "\n",
    "def create_bias(shape):\n",
    "    # creates and initializes a bias term of the specified size\n",
    "    return tf.Variable( tf.constant(0.1, shape=shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multinomial Logistic Regression (One Hidden Layer)\n",
    "\n",
    "Modify the code from the tutorial to implement a neural network with one hidden layer. In particular, we will consider the following model. Let $\\mathbf{X}$ be the matrix containing the images (with one row per image).\n",
    "\n",
    "**First layer (hidden layer).** The first layer of the NN computes the auxiliary variables $\\mathbf{Z}^{(0)}$ as\n",
    "$$\n",
    "\\mathbf{Z}^{(0)} = f\\left(\\mathbf{X}\\mathbf{W}^{(0)} + \\mathbf{b}^{(0)}\\right),\n",
    "$$\n",
    "where\n",
    "\n",
    "- $\\mathbf{W}^{(0)}$ denotes the weights of the top layer. The size of this matrix is $D\\times K$, where $D$ is the dimensionality of the inputs (i.e., the number of pixels of each image), and $K$ is the number of hidden units. You can use $K=2000$.\n",
    "\n",
    "- $\\mathbf{b}^{(0)}$ denotes the biases of the top layer (a vector of length $K$).\n",
    "\n",
    "- $f(\\cdot)$ is a non-linear function. For concreteness, we will use a rectified linear unit (ReLU), which is defined as\n",
    "$$\n",
    "f(x) = \\max(0,x)\n",
    "$$\n",
    "In Tensorflow, we can use the ReLU function `tf.nn.relu()`.\n",
    "\n",
    "*Note:* $\\mathbf{Z}^{(0)}$ is an auxiliary variable, but it is not a variable to be optimized. This means that it must *not* be declared in block b (Variables) of the computational graph. The only variables that must appear in block b (Variables) are the weights and biases of both layers.\n",
    "\n",
    "**Second layer (output layer).** The second layer of the NN is a softmax layer like the one implemented in the previous tutorial. The only difference is that the input to this layer is $\\mathbf{Z}^{(0)}$ instead of $\\mathbf{X}$. That is,\n",
    "$$\n",
    "p(y_n=j\\;|\\; \\mathbf{z}_n^{(0)}) = \\frac{e^{{\\mathbf{z}_n^{(0)}}\\mathbf{w}_j^{(1)} + b_j^{(1)}} }{\\sum_{j^\\prime=1}^J e^{{\\mathbf{z}_n^{(0)}}\\mathbf{w}_{j^\\prime}^{(1)} + b_{j^\\prime}^{(1)} }},\\qquad j=1,\\ldots,J\n",
    "$$\n",
    "Similarly to the tutorial, we need $J$ weight vectors $\\mathbf{w}_j^{(1)}$ (each of lengts $K$) and $J$ biases $b_j^{(1)}$, which we can group together in a $K\\times J$ matrix $\\mathbf{W}^{(1)}$ and a $J$-vector $\\mathbf{b}^{(1)}$. This is as shown in the tutorial; the only difference now is the size of $\\mathbf{W}^{(1)}$, which is now $K\\times J$ instead of $D\\times J$ to match the size of the input $\\mathbf{Z}^{(0)}$.\n",
    "\n",
    "**[Task]** Implement and train the network. Find the validation and test accuracy. Is it better or worse than in the tutorial?\n",
    "\n",
    "Some hints:\n",
    "\n",
    "1. Use 10000 iterations of SGD with minibatch size of 100. Use AdamOptimizer with learning rate $10^{-5}$.\n",
    "\n",
    "2. Think about how many latent variables there are and what are their sizes.\n",
    "\n",
    "3. You may use the following function that encapsulates the operations across the entire network:\n",
    "\n",
    "```python\n",
    "def all_nn_computations(X, weights_0, biases_0, weights_1, biases_1):\n",
    "    Z_0 = tf.nn.relu( tf.matmul(X, weights_0) + biases_0 )\n",
    "    logits = tf.matmul(Z_0, weights_1) + biases_1\n",
    "    return logits\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multinomial Logistic Regression (Two Hidden Layers)\n",
    "\n",
    "Now, extend the model with a second hidden layer. The resulting model is described below.\n",
    "\n",
    "**First layer (hidden layer).** We will use a standard linear layer with a ReLU output,\n",
    "$$\n",
    "\\mathbf{Z}^{(0)} = f\\left(\\mathbf{X}\\mathbf{W}^{(0)} + \\mathbf{b}^{(0)}\\right),\n",
    "$$\n",
    "where $f(\\cdot)$ is the ReLU function.\n",
    "\n",
    "**Second layer (hidden layer).** Again, we will use a standard linear layer with a ReLU output,\n",
    "$$\n",
    "\\mathbf{Z}^{(1)} = f\\left(\\mathbf{Z}^{(0)}\\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}\\right)\n",
    "$$\n",
    "\n",
    "**Third layer (output layer).** As before, we will use a softmax function,\n",
    "$$\n",
    "p(y_n=j\\;|\\; \\mathbf{z}_n^{(1)}) = \\frac{e^{{\\mathbf{z}_n^{(1)}}\\mathbf{w}_j^{(2)} + b_j^{(2)}} }{\\sum_{j^\\prime=1}^J e^{{\\mathbf{z}_n^{(1)}}\\mathbf{w}_{j^\\prime}^{(2)} + b_{j^\\prime}^{(2)} }},\\qquad j=1,\\ldots,J\n",
    "$$\n",
    "\n",
    "Now, the size of $\\mathbf{W}^{(0)}$ is $D\\times K_0$, the size of $\\mathbf{W}^{(1)}$ is $K_0\\times K_1$, and the size of $\\mathbf{W}^{(2)}$ is $K_1\\times J$. You may use $K_0=1000$ and $K_1=100$. The bias vectors $\\mathbf{b}^{(0)}$ and $\\mathbf{b}^{(1)}$ have length $K_0$ and $K_1$, respectively.\n",
    "\n",
    "**[Task]** Implement and train the network.\n",
    "\n",
    "Hints:\n",
    "\n",
    "1. Use SGD (15000 iterations, batch size of 100) and AdamOptimizer. Set the learning rate to $10^{-5}$.\n",
    "\n",
    "2. You need to re-define `all_nn_computations()`. Make sure you do that appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
