{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Language Models #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models are one of the most important concepts in Natural Language Processing (NLP). They help us assign probabilities to sequence of words, such as sentences or phrases. Their application spans beyond NLP. For example, language models help us obtain better speech recognition, optical character recognition (OCR) and information retrieval results, to name a few. Given a sentence $S$ with a set of $w_i$ words, $i=1,2,...,n$, language models formally define the probability of the sentence as the probability of having the particular sequence of words:  \n",
    "$$ \\Large p(S)=p(w_1, w_2, w_3, ... , w_n) $$  \n",
    "\n",
    "They also help us predict the probability of a specific word given the previous words in the sentence:  \n",
    "\n",
    "$$ \\Large p(w | w_{-1}, w_{-2}..w_{-k}) $$  \n",
    "\n",
    "The probability of a sentence is computed using the chain rule:  \n",
    "\n",
    "$$ \\Large p(S)=p(w_1, w_2, w_3, ... , w_n) = \\prod_i^n {p(w_i|w_1,w_2,...,w_{i-1})} $$  \n",
    "\n",
    "When computing this probability we use the Markov assumption which simplifies the computation of the above probability:    \n",
    "\n",
    "$$ \\Large p(S)=p(w_1, w_2, w_3, ... , w_n) \\approx \\prod_i^n {p(w_i|w_{i-k},w_{i-(k-1)},...,w_{i-1})} $$  \n",
    "\n",
    "The Markov assumption states that the conditional probability of a word $w_i$, given all of the previous words in the sentence, could be approximated by considering only the $k$ previous words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Models ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram Model ####\n",
    "The unigram language model is the simplest of the n-gram models. Under this model the probability of the sentence $S$ is simply a product of the probabilities of each individual word in the sentence:  \n",
    "\n",
    "$$ \\Large p(S) \\approx \\prod_i^n {p(w_i)} $$  \n",
    "\n",
    "N-gram probabilities are computed using Maximum Likelihood Estimates (MLE). For the unigram language model we compute MLE by counting the number of times word $w_i$ occurred in the collection and we divide that number by the total number of words $v$ in the collection:\n",
    "$$ \\Large p(w_i) = \\frac{count(w_i)}{\\sum_{i=1}^{v}{count(w_i)}} $$  \n",
    "\n",
    "#### Bigram Model ####\n",
    "In the bigram model the probability of a word is conditioned only on the previous word:  \n",
    "$$ \\Large p(S) \\approx \\prod_i^n {p(w_i|w_{i-1})} $$  \n",
    "\n",
    "The MLE for the bigram LM is computed by dividing the number of times words $w_i$ and $w_{i-1}$ occured together in the collection with the number of occurances of word $w_i$, $count(w_i)$ :\n",
    "\n",
    "$$ \\Large p(w_i|w_{i-1}) = \\frac{count(w_i|w_{i-1})}{count(w_i)} $$  \n",
    "\n",
    "#### Trigram Model ####\n",
    "In the trigram model the probability of a word is conditioned on the previous two words:  \n",
    "$$ \\Large p(S) \\approx \\prod_i^n {p(w_i|w_{i-1},w_{i-2})} $$  \n",
    "\n",
    "The MLE for the trigram LM is computed by dividing the number of times words $w_i$, $w_{i-1}$, $w_{i-2}$ occurred together in the collection with the count for $w_{i-1}$ and $w_{i-2}$ occurring together, $count(w_{i-1},w_{i-2})$:\n",
    "\n",
    "$$ \\Large p(w_i|w_{i-1},w_{i-2}) = \\frac{count(w_i|w_{i-1},w_{i-2})}{count(w_{i-1},w_{i-2})} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example ##\n",
    "In this task we are going to compute bigram and trigram language models using a novel. Rather than implementing language models on our own we are going to use the nltk package.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look into the books folder and choose one of the ten books that you would like to compute language models over. In the example below we are using the novel \"Siddhartha\" by Herman Hesse. Load the book and extract words and sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "book= open('./books/hesse_siddhartha.txt','r').read()\n",
    "\n",
    "words = nltk.word_tokenize(book)\n",
    "words = [word.lower() for word in words]\n",
    "sentences = nltk.sent_tokenize(book)\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences:\n",
    "\twords = nltk.word_tokenize(sentence)\n",
    "\twords = [word.lower() for word in words]\n",
    "\ttokenized_sentences.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Compute bigram language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First compute the bigrams (i.e. the tuple of words that occur together)\n",
    "bigram_model  = nltk.bigrams(words)\n",
    "#Then compute the frequency for each bigram\n",
    "bigram_frequency_word = nltk.ConditionalFreqDist(bigram_model)\n",
    "#To get the bigram probabilities we would need to normalize the bigram frequencies:\n",
    "bigram_probability_word = nltk.ConditionalProbDist(bigram_frequency_word, nltk.MLEProbDist)\n",
    "\n",
    "#Let's compute the bigram probabilities constraining on sentences:\n",
    "bigram_frequency_sent = nltk.ConditionalFreqDist((word[0],word[1]) for word in list( itertools.chain (*[nltk.bigrams(i) for i in tokenized_sentences])))\n",
    "bigram_probability_sent = nltk.ConditionalProbDist(bigram_frequency_sent, nltk.MLEProbDist)\n",
    "\n",
    "#Let's print the bigram probabilities computed over sentences:\n",
    "bigram_probability = bigram_probability_sent\n",
    "bigram_frequency = bigram_frequency_sent\n",
    "\n",
    "all_bigrams2 = {}\n",
    "for source_word in bigram_probability:\n",
    "\tprob_words = bigram_probability[source_word].samples()\n",
    "\tdenom = len(prob_words)\n",
    "\tall_bigrams2[source_word]={}\n",
    "\tfor target_word in prob_words:\n",
    "\t\tprob = bigram_probability[source_word].prob(target_word)\n",
    "\t\tall_bigrams2[source_word][target_word] = prob\n",
    "\t\tprint (\"p(\"+target_word+\"|\"+source_word+\")={0:.4f}\".format(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We could also obtain bigram probabilities for specific words:\n",
    "bigram_probability.conditions()\n",
    "query = bigram_probability[\"suffering\"].prob(\"was\")\n",
    "print (str(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Compute trigram language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First compute the trigrams:\n",
    "trigram_model  = nltk.trigrams(words)\n",
    "#Then compute the frequency for each trigram:\n",
    "trigram_words = (((word[0], word[1]), word[2]) for word in trigram_model)\n",
    "trigram_frequency_word = nltk.ConditionalFreqDist(trigram_words)\n",
    "#To get the trigram probabilities we would need to normalize the trigram frequencies:\n",
    "trigram_probability_word = nltk.ConditionalProbDist(trigram_frequency_word, nltk.MLEProbDist)\n",
    "#Let's compute the trigram probabilities constraining on sentences:\n",
    "trigram_frequency_sent = nltk.ConditionalFreqDist(((word[0], word[1]), word[2])  for word in list( itertools.chain (*[nltk.trigrams(i) for i in tokenized_sentences])))\n",
    "trigram_probability_sent = nltk.ConditionalProbDist(trigram_frequency_sent, nltk.MLEProbDist)\n",
    "\n",
    "#Let's print the trigram probabilities computed over sentences:\n",
    "trigram_probability = trigram_probability_sent\n",
    "trigram_frequency = trigram_frequency_sent\n",
    "all_trigrams2 = {}\n",
    "for source_word in trigram_probability:\n",
    "\tprob_words = trigram_probability[source_word].samples()\n",
    "\tdenom = len(prob_words)\n",
    "\tall_trigrams2[source_word]={}\n",
    "\tfor target_word in prob_words:\n",
    "\t\tprob = trigram_probability[source_word].prob(target_word)\n",
    "\t\tfreq = trigram_frequency[source_word][target_word]\n",
    "\t\tprob2 = (1.0)*freq/denom\n",
    "\t\tall_trigrams2[source_word][target_word] = prob\n",
    "\t\tprint (\"p(\"+target_word+\"|\"+source_word[0]+\",\"+source_word[1]+\")={0:.4f}\".format(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**[Assignment 1]** What about unigram language model? Using the above example code for computing bigram and trigram language models seen if you could implement the unigram language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Enter your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:p35]",
   "language": "python",
   "name": "conda-env-p35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
