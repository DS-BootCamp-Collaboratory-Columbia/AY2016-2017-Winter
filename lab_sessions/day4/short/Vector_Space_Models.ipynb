{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Models ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many tasks in information retrieval (IR) and natural language processing (NLP) involve performing document similarity comparisons. These tasks include document clustering, retrieving the most relevant documents for a given query, finding document translation pairs in a large multilingual collection, etc.  \n",
    "\n",
    "Most practical applications of document similarity represent documents in a common feature space. Representing documents in a shared feature space abstracts away from the specific sequence of words used in each document and, with appropriate representations, can also facilitate the analysis of relationships between documents written using different vocabularies.   \n",
    "\n",
    "In this part of the lab session we are going to cover one of the fundamental retrieval models - the vector space model \n",
    "In this model queries and documents are represented in a shared space whose dimensions are the number of index terms (e.g. words, n-grams, stems, phrases, etc.) More specifically, for a document $D$ and a vocabulary $V$ of index terms this representation is a vector whose dimensions are the weights for each of the index terms $|V|$.  \n",
    "\n",
    "Since document features are the weights computed over single indexed terms in a document of a few thousend words only a few hundred unique words will have non-zero counts. This makes the document representation very sparse which in turn allows for the whole document collection to be represeted as a sparse matrix  where the  raws are the documents and the columns correspond to the  indexed terms. In practice a typical representation  has the indexed terms as rows and the columns as the weights assigned to the term for a given document.  \n",
    "\n",
    "The most widely used weighing scheme for indexed terms is the tf-idf weighing. In this approach weights are computed as a product between the indexed terms frequency of occurence within the document, is reffered to as the **term frequency (tf)**, and the ** inverse document frequency (idf)** which is the log of the ratio between the total number of documents in the collection $N$ and the number of documents that contain that term $n_k$. For term  $k$ and document $i$ these  weighing components are computed using the following formulas:\n",
    "\n",
    "$$ \\Large tf_{ik} =\\frac{f_{ik}}{\\sum_{j=1}^t{tf_{ij}}} \\ \\ \\ \\ \\ \\ \\ \\ idf_k =\\log \\frac{N}{n_k}$$\n",
    "\n",
    "In this lab section we are going to represent a set of books using the td-idf model. Using the tf-idf vector representation we'll run a query and rank books based on their distance from the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import sklearn.metrics.pairwise\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "books = {}\n",
    "book_names = []\n",
    "for filename in os.listdir(\"./books\"):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        book= open(os.path.join(\"./books\",filename),'r').read().lower()\n",
    "        book = book.translate(string.punctuation)\n",
    "        books[filename]= book\n",
    "    book_names.append(filename)\n",
    "book_names = np.asarray(book_names)\n",
    "\n",
    "#Convert the collection of books into tf-idf vectors:\n",
    "tfidf = TfidfVectorizer(tokenizer=nltk.word_tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(books.values()).todense()\n",
    "\n",
    "#Now that we have the books represented in a shared vector space let's run a query:\n",
    "query='''And what was he?\n",
    "    Forsooth, a great arithmetician,\n",
    "    One Michael Cassio, a Florentine\n",
    "    (A fellow almost damn'd in a fair wife)\n",
    "    That never set a squadron in the field,\n",
    "    Nor the division of a battle knows\n",
    "    More than a spinster; unless the bookish theoric,\n",
    "    Wherein the toged consuls can propose\n",
    "    As masterly as he.'''\n",
    "query=query.lower()\n",
    "query = query.translate(string.punctuation)\n",
    "\n",
    "#We'll first represent the query in the same shared space as the books:\n",
    "query_tfidf = tfidf.transform(query).todense()\n",
    "#Next, we'll compute Euclidean distance between the query point and the books:\n",
    "eu_distances = sklearn.metrics.pairwise.euclidean_distances(tfs,query_tfidf)\n",
    "eu_distances = [row[0] for row in eu_distances]\n",
    "#Sort the array of Euclidean values:\n",
    "eu_distances_sorted = np.argsort(eu_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 1]** Using the indexes of the sorted Euclidean distances print a ranked list of the books. Hint: book filenames are stored in the book_names array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Enter your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:p35]",
   "language": "python",
   "name": "conda-env-p35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
