{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Models ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many tasks in information retrieval (IR) and natural language processing (NLP) involve performing document similarity comparisons. These tasks include document clustering, retrieving the most relevant documents for a given query, finding document translation pairs in a large multilingual collection, etc.  \n",
    "\n",
    "Most practical applications of document similarity represent documents in a common feature space. Representing documents in a shared feature space abstracts away from the specific sequence of words used in each document and, with appropriate representations, can also facilitate the analysis of relationships between documents written using different vocabularies.   \n",
    "\n",
    "In this part of the lab session we are going to cover one of the fundamental retrieval models - the vector space model \n",
    "In this model queries and documents are represented in a shared space whose dimensions are the number of index terms (e.g. words, n-grams, stems, phrases, etc.) More specifically, for a document $D$ and a vocabulary $V$ of index terms this representation is a vector whose dimensions are the weights for each of the index terms $|V|$.  \n",
    "\n",
    "Since document features are the weights computed over single indexed terms in a document of a few thousend words only a few hundred unique words will have non-zero counts. This makes the document representation very sparse which in turn allows for the whole document collection to be represeted as a sparse matrix  where the  raws are the documents and the columns correspond to the  indexed terms. In practice a typical representation  has the indexed terms as rows and the columns as the weights assigned to the term for a given document.  \n",
    "\n",
    "The most widely used weighing scheme for indexed terms is the tf-idf weighing. In this approach weights are computed as a product between the indexed terms frequency of occurence within the document, is reffered to as the **term frequency (tf)**, and the ** inverse document frequency (idf)** which is the log of the ratio between the total number of documents in the collection $N$ and the number of documents that contain that term $n_k$. For term  $k$ and document $i$ these  weighing components are computed using the following formulas:\n",
    "\n",
    "$$ \\Large tf_{ik} =\\frac{f_{ik}}{\\sum_{j=1}^t{tf_{ij}}} \\ \\ \\ \\ \\ \\ \\ \\ idf_k =\\log \\frac{N}{n_k}$$\n",
    "\n",
    "In this lab section we are going to represent a set of books using the td-idf model. Using the tf-idf vector representation we'll run a query and rank books based on their distance from the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import sklearn.metrics.pairwise\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "books = {}\n",
    "book_names = []\n",
    "for filename in sorted(os.listdir(\"./books\")):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        book= open(os.path.join(\"./books\",filename),'r').read().lower()\n",
    "        book = book.translate(string.punctuation)\n",
    "    books[filename]= book\n",
    "    book_names.append(filename)\n",
    "book_names = np.asarray(book_names)\n",
    "\n",
    "#Convert the collection of books into tf-idf vectors:\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfs = tfidf.fit_transform([books[name] for name in sorted(books.keys())])\n",
    "\n",
    "#Now that we have the books represented in a shared vector space let's run a query:\n",
    "query=['''And what was he?\n",
    "    Forsooth, a great arithmetician,\n",
    "    One Michael Cassio, a Florentine\n",
    "    (A fellow almost damn'd in a fair wife)\n",
    "    That never set a squadron in the field,\n",
    "    Nor the division of a battle knows\n",
    "    More than a spinster; unless the bookish theoric,\n",
    "    Wherein the toged consuls can propose\n",
    "    As masterly as he.'''.lower().translate(string.punctuation)]\n",
    "\n",
    "query_tfidf = tfidf.transform(query)\n",
    "#Compute Euclidean distance between the query representation in the vector space and the 10 books:\n",
    "eu_distances = sklearn.metrics.pairwise.euclidean_distances(tfs,query_tfidf)\n",
    "\n",
    "eu_distances = eu_distances.ravel()\n",
    "#Sort Euclidean distances:\n",
    "eu_distances_sorted = np.argsort(eu_distances)\n",
    "eu_distances = eu_distances[eu_distances_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 1]** Using the indexes of the sorted Euclidean distances print a ranked list of the books. Hint: book filenames are stored in the book_names array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.shakespeare_othello.txt\t1.3277\n",
      "2.may_mylifeandmyefforts.txt\t1.3996\n",
      "3.goethe_hermannanddorothea_e.txt\t1.4003\n",
      "4.goethe_faust.txt\t1.4058\n",
      "5.shakespeare_macbeth.txt\t1.4061\n",
      "6.goethe_egmont.txt\t1.4073\n",
      "7.kant_thecritiqueofpurereason.txt\t1.4079\n",
      "8.twain_theadvanturesoftomsawyer.txt\t1.4097\n",
      "9.hesse_siddhartha.txt\t1.4116\n",
      "10.kafka_transformation.txt\t1.412\n"
     ]
    }
   ],
   "source": [
    "#Sort the list of books based on the sorted Euclidean distances:\n",
    "book_names_eu = book_names[eu_distances_sorted]\n",
    "#Print the ranked list of books:\n",
    "i=1\n",
    "for sim_book in book_names_eu:\n",
    "    print(str(i)+\".\"+str(sim_book)+\"\\t\"+str(round(eu_distances[i-1],4)))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:p35]",
   "language": "python",
   "name": "conda-env-p35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
